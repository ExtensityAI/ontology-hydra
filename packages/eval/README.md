# Ontopipe Evaluation Framework

This package provides an evaluation framework for the ontopipe system, allowing you to assess the performance of ontology and knowledge graph generation on question-answering tasks.

## Features

- **Full Evaluation**: Generate ontologies and knowledge graphs, then evaluate QA performance
- **Knowledge Graph Only**: Generate knowledge graphs without ontologies for comparison (set `domain: null` in config)
- **Existing KG Evaluation**: Run evaluation on pre-existing knowledge graphs without regenerating them
- **SQuAD v2 Integration**: Uses the SQuAD v2 dataset format for evaluation
- **Caching**: Intermediate results are cached to avoid recomputation
- **Visualization**: Automatic generation of HTML visualizations for ontologies and knowledge graphs
- **Parallel Processing**: Neo4j evaluation supports parallel batch processing for faster execution

## Installation

```bash
uv sync
```

## Usage

### Full Evaluation (with Ontology)

Run a complete evaluation including ontology generation:

```bash
uv run eval new --config eval/config.json
```

### Knowledge Graph Only Evaluation (without Ontology)

To run evaluation without ontology generation, simply set `domain: null` in your configuration:

```json
{
  "scenarios": [{
    "id": "biomed_kg_only",
    "domain": null,
    "squad_titles": ["Biomedical Engineer"],
    "dataset_mode": "dev"
  }]
}
```

Then run the normal evaluation command:

```bash
uv run eval new --config eval/config_kg_only.json
```

### Evaluation on Existing Knowledge Graphs

Run evaluation on a pre-existing knowledge graph without regenerating it:

```bash
uv run eval eval-existing \
  --config eval/config.json \
  --kg-path path/to/existing/kg.json \
  --topic "Biomedical Engineer" \
  --output eval/runs/existing_kg_eval
```

This command is useful for:
- Evaluating knowledge graphs generated by other systems
- Comparing different knowledge graph generation methods
- Re-running evaluation with different parameters without regenerating the KG
- Testing knowledge graphs from external sources

**Requirements:**
- The knowledge graph must be in the same JSON format as generated by ontopipe
- The topic name must match one of the topics in your configuration file
- The configuration file must contain the appropriate dataset mode and Neo4j settings

### Resume Evaluation

Resume an interrupted evaluation run:

```bash
uv run eval resume --path eval/runs/20250101_ABC12
```

## Configuration

The evaluation framework uses JSON configuration files to define evaluation scenarios.

### Configuration Format

```json
{
  "scenarios": [
    {
      "id": "scenario_name",
      "domain": "Domain description for ontology generation (optional)",
      "squad_titles": ["Topic Title 1", "Topic Title 2"],
      "dataset_mode": "test",
      "neo4j": {
        "enabled": true,
        "uri": "bolt://localhost:7687",
        "user": "neo4j",
        "password": "ontology",
        "batch_size": 5,
        "num_iterations": 1
      }
    }
  ]
}
```

### Parameters

- `id`: Unique identifier for the scenario
- `domain`: Domain description used for ontology generation. Set to `null` to skip ontology generation
- `squad_titles`: List of SQuAD dataset topic titles to evaluate
- `dataset_mode`: Dataset to use for evaluation - `"dev"` (smaller, faster) or `"test"` (full dataset, default)
- `neo4j`: (optional) Neo4j evaluation configuration. If omitted or `enabled: false`, Neo4j evaluation is skipped.

#### Dataset Modes
- `"dev"`: Uses the development dataset (smaller, faster for testing)
- `"test"`: Uses the full test dataset (default, comprehensive evaluation)

#### Neo4j Config Options
- `enabled`: Set to `true` to enable Neo4j evaluation for this scenario
- `uri`: Neo4j connection URI (default: `bolt://localhost:7687`)
- `user`: Neo4j username (default: `neo4j`)
- `password`: Neo4j password (default: `ontology`)
- `batch_size`: Number of questions per batch (default: 5)
- `num_iterations`: Number of evaluation iterations (default: 1)
- `max_workers`: Maximum number of parallel workers for batch processing (default: 4)
- `fuzzy_threshold`: Threshold for fuzzy answer matching (default: 0.8)
- `use_run_specific_databases`: Use separate databases per evaluation run (default: true)
- `default_database`: Fallback database name (default: "neo4j")
- `auto_cleanup`: Automatically clean up old run-specific databases (default: false)

**Note**: The APOC plugin must be installed in your Neo4j database for the evaluation to work properly. You can install it via the Neo4j Desktop application or by following the [APOC installation guide](https://neo4j.com/labs/apoc/installation/).

### Example Configurations

**Development mode (fast testing):**
```json
{
  "scenarios": [{
    "id": "biomed_dev",
    "domain": "Biomedical engineering...",
    "squad_titles": ["Biomedical Engineer"],
    "dataset_mode": "dev",
    "neo4j": {
      "enabled": true
    }
  }]
}
```

**Full evaluation with test dataset and parallel processing:**
```json
{
  "scenarios": [{
    "id": "biomed_test",
    "domain": "Biomedical engineering...",
    "squad_titles": ["Biomedical Engineer"],
    "dataset_mode": "test",
    "neo4j": {
      "enabled": true,
      "max_workers": 8,
      "batch_size": 10
    }
  }]
}
```

**Knowledge graph only evaluation (no ontology, no Neo4j):**
```json
{
  "scenarios": [{
    "id": "biomed_kg_only",
    "domain": null,
    "squad_titles": ["Biomedical Engineer"],
    "dataset_mode": "dev"
  }]
}
```

## Output Structure

Each evaluation run creates a directory with the following structure:

```
eval/runs/YYYYMMDD_XXXXX/
├── config.json              # Configuration used for this run
├── logs/                    # Log files
├── ontology.html           # Ontology visualization (if domain provided)
└── topics/
    └── Topic Title/
        ├── kg.html         # Knowledge graph visualization
        ├── kg.json         # Knowledge graph data
        ├── metrics.json    # Evaluation metrics
        └── qas.json        # Question-answer pairs
```

## Evaluation Metrics

The framework computes SQuAD v2 metrics including:
- Exact Match (EM)
- F1 Score
- No-Answer Probability

## Comparison Studies

To compare ontology-guided vs. ontology-free knowledge graph generation:

1. Run full evaluation: `uv run eval new --config eval/config.json`
2. Run KG-only evaluation by setting `domain: null` in your config and running: `uv run eval new --config eval/config_kg_only.json`
3. Compare metrics in the respective `metrics.json` files

## Parallel Processing

The Neo4j evaluation supports parallel processing to speed up batch operations. You can control the level of parallelism using the `max_workers` parameter in your Neo4j configuration.

### Parallel Processing Configuration

```json
{
  "neo4j": {
    "enabled": true,
    "max_workers": 8,  // Process up to 8 batches in parallel
    "batch_size": 5    // Each batch contains 5 questions
  }
}
```

### Performance Considerations

- **CPU-bound operations**: Cypher query generation and fuzzy answer matching benefit from parallelization
- **I/O-bound operations**: Neo4j database queries are I/O-bound and can benefit from multiple concurrent connections
- **Memory usage**: Each worker maintains its own connection pool and runtime tracking
- **Optimal settings**: Start with `max_workers = 4` and adjust based on your system's capabilities

### Environment Variables

You can also control parallelization via environment variables:

```bash
export NEO4J_MAX_WORKERS=8
export NEO4J_BATCH_SIZE=10
uv run eval new --config eval/config.json
```

## Customization

### Batch Sizes

Modify `KG_BATCH_SIZE` and `QA_BATCH_SIZE` in `src/eval/eval.py` to adjust processing batch sizes.

### Model Configuration

Update the model used for question answering in the `_answer_questions` function.

### Visualization Options

Customize visualization parameters in `src/ontopipe/vis.py`.
