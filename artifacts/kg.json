{
  "0": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "Return Decomposition Technique"
    },
    "confidence": 0.85
  },
  "1": {
    "subject": {
      "name": "Markov Decision Process"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "Optimal Reward Redistribution"
    },
    "confidence": 0.9
  },
  "2": {
    "subject": {
      "name": "Reward Redistribution Method"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "Sequence-Markov Decision Process"
    },
    "confidence": 0.8
  },
  "3": {
    "subject": {
      "name": "Return Decomposition Technique"
    },
    "predicate": {
      "name": "integrates"
    },
    "object": {
      "name": "Contribution Analysis"
    },
    "confidence": 0.75
  },
  "4": {
    "subject": {
      "name": "Delayed Reward Mechanism"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "Delayed Reward Mechanism"
    },
    "confidence": 0.7
  },
  "5": {
    "subject": {
      "name": "Reward Redistribution Method"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Return Decomposition Technique"
    },
    "confidence": 0.85
  },
  "6": {
    "subject": {
      "name": "Return Decomposition Technique"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "Q-learning Method"
    },
    "confidence": 0.8
  },
  "7": {
    "subject": {
      "name": "Advantage Function"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "Reward Redistribution Method"
    },
    "confidence": 0.75
  },
  "8": {
    "subject": {
      "name": "Temporal Value Transport Mechanism"
    },
    "predicate": {
      "name": "integrates"
    },
    "object": {
      "name": "Return Decomposition Technique"
    },
    "confidence": 0.78
  },
  "9": {
    "subject": {
      "name": "Markov Decision Process"
    },
    "predicate": {
      "name": "described by"
    },
    "object": {
      "name": "Reward Redistribution Method"
    },
    "confidence": 0.8
  },
  "10": {
    "subject": {
      "name": "Sequence-Markov Decision Process"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Markov Decision Process"
    },
    "confidence": 0.75
  },
  "11": {
    "subject": {
      "name": "Reward Redistribution Method"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "Sequence-Markov Decision Process"
    },
    "confidence": 0.85
  },
  "12": {
    "subject": {
      "name": "Optimal Reward Redistribution"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "expected future rewards"
    },
    "confidence": 0.9
  },
  "13": {
    "subject": {
      "name": "Markov Decision Process"
    },
    "predicate": {
      "name": "integrates"
    },
    "object": {
      "name": "Reward Redistribution Method"
    },
    "confidence": 0.8
  },
  "14": {
    "subject": {
      "name": "Markov Decision Process"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "Sequence-Markov Decision Process"
    },
    "confidence": 0.85
  },
  "15": {
    "subject": {
      "name": "Optimal Reward Redistribution"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Markov Decision Process"
    },
    "confidence": 0.78
  },
  "16": {
    "subject": {
      "name": "Delayed Reward Mechanism"
    },
    "predicate": {
      "name": "minimizes"
    },
    "object": {
      "name": "expected future rewards"
    },
    "confidence": 0.8
  },
  "17": {
    "subject": {
      "name": "Bellman Equation"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "Q-value estimation"
    },
    "confidence": 0.9
  },
  "18": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Long Short-Term Memory Network"
    },
    "confidence": 0.85
  },
  "19": {
    "subject": {
      "name": "RUDDER"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.8
  },
  "20": {
    "subject": {
      "name": "Atari games"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "Value Function"
    },
    "confidence": 0.75
  },
  "21": {
    "subject": {
      "name": "RUDDER"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Reward Redistribution Method"
    },
    "confidence": 0.9
  },
  "22": {
    "subject": {
      "name": "Human Expert Episodes"
    },
    "predicate": {
      "name": "contributes to"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.82
  },
  "23": {
    "subject": {
      "name": "RUDDER"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "Q(\u03bb)"
    },
    "confidence": 0.95
  },
  "24": {
    "subject": {
      "name": "RUDDER"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "MC"
    },
    "confidence": 0.95
  },
  "25": {
    "subject": {
      "name": "RUDDER"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "MCTS"
    },
    "confidence": 0.95
  },
  "26": {
    "subject": {
      "name": "RUDDER"
    },
    "predicate": {
      "name": "supports"
    },
    "object": {
      "name": "Supplements Theorem S8"
    },
    "confidence": 0.85
  },
  "27": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "PPO baseline"
    },
    "confidence": 0.9
  },
  "28": {
    "subject": {
      "name": "RUDDER"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "TD(\u03bb)"
    },
    "confidence": 0.95
  },
  "29": {
    "subject": {
      "name": "RUDDER"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "MC"
    },
    "confidence": 0.95
  },
  "30": {
    "subject": {
      "name": "RUDDER"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "MCTS"
    },
    "confidence": 0.95
  },
  "31": {
    "subject": {
      "name": "RUDDER"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "reward shaping methods"
    },
    "confidence": 0.95
  },
  "32": {
    "subject": {
      "name": "RUDDER"
    },
    "predicate": {
      "name": "increases"
    },
    "object": {
      "name": "scores"
    },
    "confidence": 0.85
  },
  "33": {
    "subject": {
      "name": "RUDDER"
    },
    "predicate": {
      "name": "controls"
    },
    "object": {
      "name": "learning time"
    },
    "confidence": 0.9
  },
  "34": {
    "subject": {
      "name": "RUDDER"
    },
    "predicate": {
      "name": "utilizes"
    },
    "object": {
      "name": "reward redistribution"
    },
    "confidence": 0.8
  },
  "35": {
    "subject": {
      "name": "Delayed Reward Mechanism"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "scores"
    },
    "confidence": 0.9
  },
  "36": {
    "subject": {
      "name": "Feudal Network"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "Hierarchical Reinforcement Learning"
    },
    "confidence": 0.85
  },
  "37": {
    "subject": {
      "name": "Manager Module"
    },
    "predicate": {
      "name": "operates_at"
    },
    "object": {
      "name": "lower temporal resolution"
    },
    "confidence": 0.9
  },
  "38": {
    "subject": {
      "name": "Worker Module"
    },
    "predicate": {
      "name": "generates"
    },
    "object": {
      "name": "primitive actions"
    },
    "confidence": 0.88
  },
  "39": {
    "subject": {
      "name": "Feudal Network"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "temporal sub-policies"
    },
    "confidence": 0.8
  },
  "40": {
    "subject": {
      "name": "Manager Module"
    },
    "predicate": {
      "name": "sets"
    },
    "object": {
      "name": "abstract goals"
    },
    "confidence": 0.92
  },
  "41": {
    "subject": {
      "name": "Worker Module"
    },
    "predicate": {
      "name": "enacts"
    },
    "object": {
      "name": "goals"
    },
    "confidence": 0.87
  },
  "42": {
    "subject": {
      "name": "Goal"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "Dilated LSTM"
    },
    "confidence": 0.85
  },
  "43": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "Transition Policy"
    },
    "confidence": 0.8
  },
  "44": {
    "subject": {
      "name": "Action Primitive"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Goal"
    },
    "confidence": 0.75
  },
  "45": {
    "subject": {
      "name": "Recurrent Neural Network"
    },
    "predicate": {
      "name": "integrates"
    },
    "object": {
      "name": "Dilated LSTM"
    },
    "confidence": 0.9
  },
  "46": {
    "subject": {
      "name": "Task"
    },
    "predicate": {
      "name": "correlates"
    },
    "object": {
      "name": "Goal"
    },
    "confidence": 0.88
  },
  "47": {
    "subject": {
      "name": "FuN"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Feudal Network"
    },
    "confidence": 0.85
  },
  "48": {
    "subject": {
      "name": "Manager Module"
    },
    "predicate": {
      "name": "controls"
    },
    "object": {
      "name": "Goal Embedding"
    },
    "confidence": 0.75
  },
  "49": {
    "subject": {
      "name": "Worker Module"
    },
    "predicate": {
      "name": "produces"
    },
    "object": {
      "name": "actions"
    },
    "confidence": 0.8
  },
  "50": {
    "subject": {
      "name": "Goal"
    },
    "predicate": {
      "name": "modulates"
    },
    "object": {
      "name": "policy"
    },
    "confidence": 0.7
  },
  "51": {
    "subject": {
      "name": "Manager Module"
    },
    "predicate": {
      "name": "outputs"
    },
    "object": {
      "name": "goal vector"
    },
    "confidence": 0.78
  },
  "52": {
    "subject": {
      "name": "Worker Module"
    },
    "predicate": {
      "name": "works with"
    },
    "object": {
      "name": "Manager Module"
    },
    "confidence": 0.77
  },
  "53": {
    "subject": {
      "name": "FuN"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "LSTM"
    },
    "confidence": 0.85
  },
  "54": {
    "subject": {
      "name": "T-maze+"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Non-match"
    },
    "confidence": 0.75
  },
  "55": {
    "subject": {
      "name": "Ablative analysis"
    },
    "predicate": {
      "name": "validates"
    },
    "object": {
      "name": "transition policy gradient for training the Manager"
    },
    "confidence": 0.8
  },
  "56": {
    "subject": {
      "name": "LSTM"
    },
    "predicate": {
      "name": "fails to utilize"
    },
    "object": {
      "name": "memory for water maze task"
    },
    "confidence": 0.7
  },
  "57": {
    "subject": {
      "name": "Feudal Network"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.85
  },
  "58": {
    "subject": {
      "name": "Manager Module"
    },
    "predicate": {
      "name": "modulates"
    },
    "object": {
      "name": "Worker Module"
    },
    "confidence": 0.8
  },
  "59": {
    "subject": {
      "name": "Transition Policy"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "Worker Module"
    },
    "confidence": 0.75
  },
  "60": {
    "subject": {
      "name": "Gaussian Distribution"
    },
    "predicate": {
      "name": "describes"
    },
    "object": {
      "name": "Policy Gradient Method"
    },
    "confidence": 0.78
  },
  "61": {
    "subject": {
      "name": "Intrinsic Reward"
    },
    "predicate": {
      "name": "enhances"
    },
    "object": {
      "name": "Feudal Network"
    },
    "confidence": 0.82
  },
  "62": {
    "subject": {
      "name": "FuN"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "Manager Module"
    },
    "confidence": 0.8
  },
  "63": {
    "subject": {
      "name": "FuN"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "LSTM"
    },
    "confidence": 0.9
  },
  "64": {
    "subject": {
      "name": "Manager Module"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Transition Policy"
    },
    "confidence": 0.85
  },
  "65": {
    "subject": {
      "name": "Worker Module"
    },
    "predicate": {
      "name": "integrates"
    },
    "object": {
      "name": "Primitive Actions"
    },
    "confidence": 0.75
  },
  "66": {
    "subject": {
      "name": "Transition Policy"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Behavioural Primitive"
    },
    "confidence": 0.8
  },
  "67": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Unsupervised visual representation learning"
    },
    "confidence": 0.85
  },
  "68": {
    "subject": {
      "name": "Reward Redistribution Method"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "the challenge of computationally intensive training"
    },
    "confidence": 0.8
  },
  "69": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "better methods for exploring pretraining algorithms"
    },
    "confidence": 0.75
  },
  "70": {
    "subject": {
      "name": "Evaluation Protocol"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "existing self-supervised learning recipes"
    },
    "confidence": 0.9
  },
  "71": {
    "subject": {
      "name": "Linear Probing Task"
    },
    "predicate": {
      "name": "correlates"
    },
    "object": {
      "name": "actual downstream control performance"
    },
    "confidence": 0.88
  },
  "72": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "correlates"
    },
    "object": {
      "name": "Reward Probing Method"
    },
    "confidence": 0.85
  },
  "73": {
    "subject": {
      "name": "Reward Probing Method"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "Linear Probing Task"
    },
    "confidence": 0.8
  },
  "74": {
    "subject": {
      "name": "Unsupervised Learning Method"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.75
  },
  "75": {
    "subject": {
      "name": "Reward Predicting Method"
    },
    "predicate": {
      "name": "correlates"
    },
    "object": {
      "name": "Value Prediction"
    },
    "confidence": 0.85
  },
  "76": {
    "subject": {
      "name": "Reward Prediction"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Value-Based Control"
    },
    "confidence": 0.75
  },
  "77": {
    "subject": {
      "name": "Reward Prediction"
    },
    "predicate": {
      "name": "supports"
    },
    "object": {
      "name": "Critic in Actor-Critic Methods"
    },
    "confidence": 0.8
  },
  "78": {
    "subject": {
      "name": "Data Collection Methodology"
    },
    "predicate": {
      "name": "is essential for"
    },
    "object": {
      "name": "Action Prediction"
    },
    "confidence": 0.9
  },
  "79": {
    "subject": {
      "name": "Transition Model"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "Representation Learning"
    },
    "confidence": 0.78
  },
  "80": {
    "subject": {
      "name": "Recurrent Neural Network"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "probing performance"
    },
    "confidence": 0.85
  },
  "81": {
    "subject": {
      "name": "Barlow"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "probing scores"
    },
    "confidence": 0.78
  },
  "82": {
    "subject": {
      "name": "GRU-latent"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "representation learning"
    },
    "confidence": 0.8
  },
  "83": {
    "subject": {
      "name": "Demon attack"
    },
    "predicate": {
      "name": "exhibits"
    },
    "object": {
      "name": "stochastic behaviours"
    },
    "confidence": 0.76
  },
  "84": {
    "subject": {
      "name": "GRU-latent"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "better predictions"
    },
    "confidence": 0.82
  },
  "85": {
    "subject": {
      "name": "latent variable"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "future predictions"
    },
    "confidence": 0.79
  },
  "86": {
    "subject": {
      "name": "BYOL"
    },
    "predicate": {
      "name": "augments"
    },
    "object": {
      "name": "reward probing"
    },
    "confidence": 0.77
  },
  "87": {
    "subject": {
      "name": "Reward Probing Method"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Pretraining Setup"
    },
    "confidence": 0.8
  },
  "88": {
    "subject": {
      "name": "Reward Probing Method"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "pretraining setups"
    },
    "confidence": 0.85
  },
  "89": {
    "subject": {
      "name": "success"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "reward probing task"
    },
    "confidence": 0.75
  },
  "90": {
    "subject": {
      "name": "reward prediction"
    },
    "predicate": {
      "name": "influences"
    },
    "object": {
      "name": "prediction task"
    },
    "confidence": 0.7
  },
  "91": {
    "subject": {
      "name": "pretraining methodology"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "reinforcement learning performance"
    },
    "confidence": 0.9
  },
  "92": {
    "subject": {
      "name": "results"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "downstream performance"
    },
    "confidence": 0.8
  },
  "93": {
    "subject": {
      "name": "Neural predictive belief representations"
    },
    "predicate": {
      "name": "explains"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.85
  },
  "94": {
    "subject": {
      "name": "Mastering atari with discrete world models"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.75
  },
  "95": {
    "subject": {
      "name": "Deep reinforcement learning that matters"
    },
    "predicate": {
      "name": "promotes"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.8
  },
  "96": {
    "subject": {
      "name": "Rainbow: Combining improvements in deep reinforcement learning"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.9
  },
  "97": {
    "subject": {
      "name": "Reward"
    },
    "predicate": {
      "name": "predicts"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.85
  },
  "98": {
    "subject": {
      "name": "Reward Prediction F1 Score"
    },
    "predicate": {
      "name": "correlates"
    },
    "object": {
      "name": "Mean RL Performance"
    },
    "confidence": 0.8
  },
  "99": {
    "subject": {
      "name": "Reward Prediction F1 Score"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "Accurate Low-Variance Estimate"
    },
    "confidence": 0.75
  },
  "100": {
    "subject": {
      "name": "AtariARI"
    },
    "predicate": {
      "name": "compares"
    },
    "object": {
      "name": "Domain Specific Probing Benchmarks"
    },
    "confidence": 0.7
  },
  "101": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Video Game Environment"
    },
    "confidence": 0.8
  },
  "102": {
    "subject": {
      "name": "MineRL Competition"
    },
    "predicate": {
      "name": "promotes"
    },
    "object": {
      "name": "Sample E\ufb03cient Reinforcement Learning"
    },
    "confidence": 0.75
  },
  "103": {
    "subject": {
      "name": "Human Prior"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.85
  },
  "104": {
    "subject": {
      "name": "ObtainDiamond task"
    },
    "predicate": {
      "name": "requires"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.7
  },
  "105": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "applies"
    },
    "object": {
      "name": "self-driving vehicles"
    },
    "confidence": 0.85
  },
  "106": {
    "subject": {
      "name": "Imitation Learning"
    },
    "predicate": {
      "name": "reduces"
    },
    "object": {
      "name": "environment sample-complexity"
    },
    "confidence": 0.9
  },
  "107": {
    "subject": {
      "name": "Deep Q-Network"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "sample-efficient methods"
    },
    "confidence": 0.8
  },
  "108": {
    "subject": {
      "name": "Minecraft"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "real-world robotics"
    },
    "confidence": 0.9
  },
  "109": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "Performance Metric"
    },
    "confidence": 0.85
  },
  "110": {
    "subject": {
      "name": "Expert Demonstration"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "higher reward per episode"
    },
    "confidence": 0.9
  },
  "111": {
    "subject": {
      "name": "Human Prior"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Generalized Advantage Estimation"
    },
    "confidence": 0.8
  },
  "112": {
    "subject": {
      "name": "Competition"
    },
    "predicate": {
      "name": "provides"
    },
    "object": {
      "name": "baselines on state of the art RL algorithms"
    },
    "confidence": 0.75
  },
  "113": {
    "subject": {
      "name": "Agent"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Expert Agent"
    },
    "confidence": 0.7
  },
  "114": {
    "subject": {
      "name": "Ruslan Salakhutdinov"
    },
    "predicate": {
      "name": "is affiliated with"
    },
    "object": {
      "name": "University of Toronto"
    },
    "confidence": 0.9
  },
  "115": {
    "subject": {
      "name": "Ruslan Salakhutdinov"
    },
    "predicate": {
      "name": "is affiliated with"
    },
    "object": {
      "name": "Carnegie Mellon University"
    },
    "confidence": 0.9
  },
  "116": {
    "subject": {
      "name": "Nicholay Topin"
    },
    "predicate": {
      "name": "is advised by"
    },
    "object": {
      "name": "Manuela Veloso"
    },
    "confidence": 0.85
  },
  "117": {
    "subject": {
      "name": "Manuela Veloso"
    },
    "predicate": {
      "name": "is affiliated with"
    },
    "object": {
      "name": "Carnegie Mellon University"
    },
    "confidence": 0.95
  },
  "118": {
    "subject": {
      "name": "Manuela Veloso"
    },
    "predicate": {
      "name": "is affiliated with"
    },
    "object": {
      "name": "JPMorgan Chase"
    },
    "confidence": 0.85
  },
  "119": {
    "subject": {
      "name": "Chelsea Finn"
    },
    "predicate": {
      "name": "is affiliated with"
    },
    "object": {
      "name": "Google Brain"
    },
    "confidence": 0.9
  },
  "120": {
    "subject": {
      "name": "Chelsea Finn"
    },
    "predicate": {
      "name": "is affiliated with"
    },
    "object": {
      "name": "UC Berkeley"
    },
    "confidence": 0.9
  },
  "121": {
    "subject": {
      "name": "Sergey Levine"
    },
    "predicate": {
      "name": "is affiliated with"
    },
    "object": {
      "name": "UC Berkeley"
    },
    "confidence": 0.9
  },
  "122": {
    "subject": {
      "name": "Harm van Seijen"
    },
    "predicate": {
      "name": "affiliated with"
    },
    "object": {
      "name": "Microsoft Research"
    },
    "confidence": 0.85
  },
  "123": {
    "subject": {
      "name": "Oriol Vinyals"
    },
    "predicate": {
      "name": "affiliated with"
    },
    "object": {
      "name": "Google DeepMind"
    },
    "confidence": 0.85
  },
  "124": {
    "subject": {
      "name": "Microsoft Research"
    },
    "predicate": {
      "name": "collaborates with"
    },
    "object": {
      "name": "academic researchers"
    },
    "confidence": 0.75
  },
  "125": {
    "subject": {
      "name": "Microsoft Research"
    },
    "predicate": {
      "name": "provides"
    },
    "object": {
      "name": "cloud computing resource"
    },
    "confidence": 0.9
  },
  "126": {
    "subject": {
      "name": "Microsoft Research"
    },
    "predicate": {
      "name": "supports"
    },
    "object": {
      "name": "competition"
    },
    "confidence": 0.9
  },
  "127": {
    "subject": {
      "name": "Preferred Networks, Inc."
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "company behind Chainer"
    },
    "confidence": 0.8
  },
  "128": {
    "subject": {
      "name": "value equivalence (VE)"
    },
    "predicate": {
      "name": "defines"
    },
    "object": {
      "name": "proper value equivalence (PVE)"
    },
    "confidence": 0.85
  },
  "129": {
    "subject": {
      "name": "proper value equivalence (PVE)"
    },
    "predicate": {
      "name": "contains"
    },
    "object": {
      "name": "multiple models"
    },
    "confidence": 0.8
  },
  "130": {
    "subject": {
      "name": "agent"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "optimal policy"
    },
    "confidence": 0.9
  },
  "131": {
    "subject": {
      "name": "value equivalence (VE)"
    },
    "predicate": {
      "name": "shrinks"
    },
    "object": {
      "name": "set of VE models"
    },
    "confidence": 0.75
  },
  "132": {
    "subject": {
      "name": "Bellman operators"
    },
    "predicate": {
      "name": "induce"
    },
    "object": {
      "name": "correct result"
    },
    "confidence": 0.88
  },
  "133": {
    "subject": {
      "name": "Proper Value Equivalence Model"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Value Function"
    },
    "confidence": 0.85
  },
  "134": {
    "subject": {
      "name": "Order-k Value Equivalence"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Value Equivalence Class"
    },
    "confidence": 0.8
  },
  "135": {
    "subject": {
      "name": "Markov Decision Process"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "Bellman Operator"
    },
    "confidence": 0.75
  },
  "136": {
    "subject": {
      "name": "Bellman Equation"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.9
  },
  "137": {
    "subject": {
      "name": "Optimal Reward Redistribution"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "Reward Redistribution Method"
    },
    "confidence": 0.85
  },
  "138": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "MuZero"
    },
    "confidence": 0.9
  },
  "139": {
    "subject": {
      "name": "Model"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "performance"
    },
    "confidence": 0.85
  },
  "140": {
    "subject": {
      "name": "Behavior Policy"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "success in the environment"
    },
    "confidence": 0.8
  },
  "141": {
    "subject": {
      "name": "Monte-Carlo Tree Search"
    },
    "predicate": {
      "name": "integrates"
    },
    "object": {
      "name": "value function estimates"
    },
    "confidence": 0.75
  },
  "142": {
    "subject": {
      "name": "Ring Markov Decision Process"
    },
    "predicate": {
      "name": "relies on"
    },
    "object": {
      "name": "Reward Redistribution Method"
    },
    "confidence": 0.85
  },
  "143": {
    "subject": {
      "name": "False-ring Markov Decision Process"
    },
    "predicate": {
      "name": "mimics"
    },
    "object": {
      "name": "Ring Markov Decision Process"
    },
    "confidence": 0.8
  },
  "144": {
    "subject": {
      "name": "False-ring Markov Decision Process"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Ring Markov Decision Process"
    },
    "confidence": 0.87
  },
  "145": {
    "subject": {
      "name": "n-step Bellman Operator"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Optimal Reward Redistribution"
    },
    "confidence": 0.9
  },
  "146": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "Reward Redistribution Method"
    },
    "confidence": 0.85
  },
  "147": {
    "subject": {
      "name": "Episodic Reward"
    },
    "predicate": {
      "name": "decomposes"
    },
    "object": {
      "name": "Return Decomposition Technique"
    },
    "confidence": 0.75
  },
  "148": {
    "subject": {
      "name": "Deep Neural Networks"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "Policy Optimization"
    },
    "confidence": 0.9
  },
  "149": {
    "subject": {
      "name": "Temporal Credit Assignment"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "Delayed Reward Mechanism"
    },
    "confidence": 0.8
  },
  "150": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "Reward Function"
    },
    "confidence": 0.85
  },
  "151": {
    "subject": {
      "name": "Markov Decision Process"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Optimal Reward Redistribution"
    },
    "confidence": 0.8
  },
  "152": {
    "subject": {
      "name": "Sequence-Markov Decision Process"
    },
    "predicate": {
      "name": "supports"
    },
    "object": {
      "name": "Transformer Model"
    },
    "confidence": 0.75
  },
  "153": {
    "subject": {
      "name": "Reward Redistribution Method"
    },
    "predicate": {
      "name": "decomposes"
    },
    "object": {
      "name": "Reward Function"
    },
    "confidence": 0.78
  },
  "154": {
    "subject": {
      "name": "Proximal Policy Optimization (PPO)"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "state-the-art performance"
    },
    "confidence": 0.85
  },
  "155": {
    "subject": {
      "name": "Proximal Policy Optimization (PPO)"
    },
    "predicate": {
      "name": "regularizes"
    },
    "object": {
      "name": "policy gradient"
    },
    "confidence": 0.75
  },
  "156": {
    "subject": {
      "name": "generalized advantage estimation (GAE)"
    },
    "predicate": {
      "name": "helps"
    },
    "object": {
      "name": "reducing variance"
    },
    "confidence": 0.8
  },
  "157": {
    "subject": {
      "name": "policy gradient methods"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "value function estimation"
    },
    "confidence": 0.7
  },
  "158": {
    "subject": {
      "name": "policy gradient methods"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "policy improvement"
    },
    "confidence": 0.72
  },
  "159": {
    "subject": {
      "name": "trajectory"
    },
    "predicate": {
      "name": "captures"
    },
    "object": {
      "name": "final state"
    },
    "confidence": 0.75
  },
  "160": {
    "subject": {
      "name": "Proximal Point Algorithm"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "policy parameters"
    },
    "confidence": 0.85
  },
  "161": {
    "subject": {
      "name": "Transformer Model"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "Language Model"
    },
    "confidence": 0.85
  },
  "162": {
    "subject": {
      "name": "Transformer Model"
    },
    "predicate": {
      "name": "integrates"
    },
    "object": {
      "name": "Recurrent Neural Network"
    },
    "confidence": 0.75
  },
  "163": {
    "subject": {
      "name": "Transformer Model"
    },
    "predicate": {
      "name": "uses"
    },
    "object": {
      "name": "multi-head attention layer"
    },
    "confidence": 0.9
  },
  "164": {
    "subject": {
      "name": "multi-head attention layer"
    },
    "predicate": {
      "name": "outputs"
    },
    "object": {
      "name": "hidden representation"
    },
    "confidence": 0.8
  },
  "165": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "influences"
    },
    "object": {
      "name": "Neural Network Model"
    },
    "confidence": 0.85
  },
  "166": {
    "subject": {
      "name": "PPO algorithm"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "Reward Function"
    },
    "confidence": 0.9
  },
  "167": {
    "subject": {
      "name": "Generalized Advantage Estimation"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "Policy Gradient Method"
    },
    "confidence": 0.88
  },
  "168": {
    "subject": {
      "name": "Environment"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Reward Redistribution Method"
    },
    "confidence": 0.75
  },
  "169": {
    "subject": {
      "name": "Simulator"
    },
    "predicate": {
      "name": "provides"
    },
    "object": {
      "name": "Dataset"
    },
    "confidence": 0.8
  },
  "170": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "Return Decomposition Technique"
    },
    "confidence": 0.85
  },
  "171": {
    "subject": {
      "name": "Return Decomposition Technique"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "episodic return baselines"
    },
    "confidence": 0.9
  },
  "172": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "Credit Assignment Algorithm"
    },
    "confidence": 0.75
  },
  "173": {
    "subject": {
      "name": "Transformer Model"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "LSTM"
    },
    "confidence": 0.88
  },
  "174": {
    "subject": {
      "name": "Credit Assignment Algorithm"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "sample efficiency"
    },
    "confidence": 0.8
  },
  "175": {
    "subject": {
      "name": "Reward Function"
    },
    "predicate": {
      "name": "integrates"
    },
    "object": {
      "name": "reinforcement learning techniques"
    },
    "confidence": 0.7
  },
  "176": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "sample efficiency"
    },
    "confidence": 0.85
  },
  "177": {
    "subject": {
      "name": "Intrinsic Reward"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "training algorithms"
    },
    "confidence": 0.75
  },
  "178": {
    "subject": {
      "name": "Credit Assignment Algorithm"
    },
    "predicate": {
      "name": "provides"
    },
    "object": {
      "name": "insights"
    },
    "confidence": 0.8
  },
  "179": {
    "subject": {
      "name": "Hindsight Experience Replay"
    },
    "predicate": {
      "name": "adds"
    },
    "object": {
      "name": "additional goals"
    },
    "confidence": 0.7
  },
  "180": {
    "subject": {
      "name": "Reward Shaping Method"
    },
    "predicate": {
      "name": "explores"
    },
    "object": {
      "name": "reward function modifications"
    },
    "confidence": 0.72
  },
  "181": {
    "subject": {
      "name": "Temporal Value Transport Mechanism"
    },
    "predicate": {
      "name": "relies on"
    },
    "object": {
      "name": "memory reconstitutive module"
    },
    "confidence": 0.71
  },
  "182": {
    "subject": {
      "name": "Value Equivalence Principle"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Proper Value Equivalence Model"
    },
    "confidence": 0.9
  },
  "183": {
    "subject": {
      "name": "Value Equivalence Principle"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.8
  },
  "184": {
    "subject": {
      "name": "Bellman Operator"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "Value Function"
    },
    "confidence": 0.85
  },
  "185": {
    "subject": {
      "name": "Bellman Operator"
    },
    "predicate": {
      "name": "modulates"
    },
    "object": {
      "name": "Q-learning Method"
    },
    "confidence": 0.75
  },
  "186": {
    "subject": {
      "name": "Value Equivalence Principle"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "Optimal Reward Redistribution"
    },
    "confidence": 0.8
  },
  "187": {
    "subject": {
      "name": "Policies"
    },
    "predicate": {
      "name": "influences"
    },
    "object": {
      "name": "Model Capacity Constraint"
    },
    "confidence": 0.7
  },
  "188": {
    "subject": {
      "name": "value-equivalent models"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "functionally identical"
    },
    "confidence": 0.9
  },
  "189": {
    "subject": {
      "name": "model m"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "Bellman operators T\u03c0"
    },
    "confidence": 0.85
  },
  "190": {
    "subject": {
      "name": "space M(\u03a0,V)"
    },
    "predicate": {
      "name": "contains"
    },
    "object": {
      "name": "all models in M"
    },
    "confidence": 0.88
  },
  "191": {
    "subject": {
      "name": "set of policies \u03a0"
    },
    "predicate": {
      "name": "corresponds to"
    },
    "object": {
      "name": "perfect model"
    },
    "confidence": 0.75
  },
  "192": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "utilizes"
    },
    "object": {
      "name": "Markov Decision Process"
    },
    "confidence": 0.85
  },
  "193": {
    "subject": {
      "name": "Markov Decision Process"
    },
    "predicate": {
      "name": "influences"
    },
    "object": {
      "name": "Value Function"
    },
    "confidence": 0.8
  },
  "194": {
    "subject": {
      "name": "Value Function"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Bellman Equation"
    },
    "confidence": 0.75
  },
  "195": {
    "subject": {
      "name": "Bellman Equation"
    },
    "predicate": {
      "name": "provides"
    },
    "object": {
      "name": "Optimal Reward Redistribution"
    },
    "confidence": 0.78
  },
  "196": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "Markov Decision Process"
    },
    "confidence": 0.8
  },
  "197": {
    "subject": {
      "name": "Value Function"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Bellman Equation"
    },
    "confidence": 0.7
  },
  "198": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "Reward Redistribution Method"
    },
    "confidence": 0.71
  },
  "199": {
    "subject": {
      "name": "Value Equivalence Principle"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "Control Strategy"
    },
    "confidence": 0.75
  },
  "200": {
    "subject": {
      "name": "V AML"
    },
    "predicate": {
      "name": "complements"
    },
    "object": {
      "name": "ours"
    },
    "confidence": 0.85
  },
  "201": {
    "subject": {
      "name": "Joseph et al."
    },
    "predicate": {
      "name": "proposes"
    },
    "object": {
      "name": "algorithm"
    },
    "confidence": 0.9
  },
  "202": {
    "subject": {
      "name": "Ayoub et al."
    },
    "predicate": {
      "name": "proposes"
    },
    "object": {
      "name": "algorithm"
    },
    "confidence": 0.92
  },
  "203": {
    "subject": {
      "name": "value-targeted regression estimation"
    },
    "predicate": {
      "name": "is sufficient for"
    },
    "object": {
      "name": "model-based RL"
    },
    "confidence": 0.8
  },
  "204": {
    "subject": {
      "name": "value equivalence principle"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "tailored models"
    },
    "confidence": 0.88
  },
  "205": {
    "subject": {
      "name": "Value Function"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Value Equivalence Class"
    },
    "confidence": 0.85
  },
  "206": {
    "subject": {
      "name": "Markov Decision Process"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "Reward Function"
    },
    "confidence": 0.8
  },
  "207": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "integrates"
    },
    "object": {
      "name": "Exploration Strategy"
    },
    "confidence": 0.75
  },
  "208": {
    "subject": {
      "name": "Optimal Adversary"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "Bellman Operator"
    },
    "confidence": 0.7
  },
  "209": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Markov Decision Process"
    },
    "confidence": 0.85
  },
  "210": {
    "subject": {
      "name": "History-Based Feature Abstraction"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.9
  },
  "211": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Markov Decision Process"
    },
    "confidence": 0.8
  },
  "212": {
    "subject": {
      "name": "Markov Decision Process"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "Reward Function"
    },
    "confidence": 0.75
  },
  "213": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Markov Decision Process"
    },
    "confidence": 0.9
  },
  "214": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "controls"
    },
    "object": {
      "name": "Continuous Control Problem"
    },
    "confidence": 0.85
  },
  "215": {
    "subject": {
      "name": "Actor Critic Algorithm"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Deep Q-Network"
    },
    "confidence": 0.88
  },
  "216": {
    "subject": {
      "name": "Policy Gradient Method"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "Memory-based Dynamic Programming Method"
    },
    "confidence": 0.87
  },
  "217": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "performance"
    },
    "confidence": 0.8
  },
  "218": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "feedforward baseline"
    },
    "confidence": 0.75
  },
  "219": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "lends credence to"
    },
    "object": {
      "name": "history-based encoding policies"
    },
    "confidence": 0.7
  },
  "220": {
    "subject": {
      "name": "Energy Distance"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "better performance"
    },
    "confidence": 0.85
  },
  "221": {
    "subject": {
      "name": "Wasserstein Distance"
    },
    "predicate": {
      "name": "compared to"
    },
    "object": {
      "name": "MMD"
    },
    "confidence": 0.9
  },
  "222": {
    "subject": {
      "name": "LSTM-based agent architecture"
    },
    "predicate": {
      "name": "results in"
    },
    "object": {
      "name": "superior performance"
    },
    "confidence": 0.78
  },
  "223": {
    "subject": {
      "name": "AIS"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "epistemic state"
    },
    "confidence": 0.85
  },
  "224": {
    "subject": {
      "name": "AIS"
    },
    "predicate": {
      "name": "represents"
    },
    "object": {
      "name": "environment proxy \u03d2"
    },
    "confidence": 0.8
  },
  "225": {
    "subject": {
      "name": "entropic measures"
    },
    "predicate": {
      "name": "captures"
    },
    "object": {
      "name": "system\u2019s uncertainty"
    },
    "confidence": 0.75
  },
  "226": {
    "subject": {
      "name": "bisimulation metrics"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "operator on the space of semi-metrics"
    },
    "confidence": 0.9
  },
  "227": {
    "subject": {
      "name": "history-based policies"
    },
    "predicate": {
      "name": "extends"
    },
    "object": {
      "name": "DeepMDP framework"
    },
    "confidence": 0.88
  },
  "228": {
    "subject": {
      "name": "RL algorithms"
    },
    "predicate": {
      "name": "uses"
    },
    "object": {
      "name": "attention mechanism"
    },
    "confidence": 0.92
  },
  "229": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Markov Decision Process"
    },
    "confidence": 0.8
  },
  "230": {
    "subject": {
      "name": "Markov Decision Process"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.7
  },
  "231": {
    "subject": {
      "name": "Policy Gradient Method"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.75
  },
  "232": {
    "subject": {
      "name": "Deep Q-Network"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "Markov Decision Process"
    },
    "confidence": 0.85
  },
  "233": {
    "subject": {
      "name": "Generative Adversarial Network"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.78
  },
  "234": {
    "subject": {
      "name": "Reinforcement Learning"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "Imitation Learning"
    },
    "confidence": 0.85
  },
  "235": {
    "subject": {
      "name": "SQIL Algorithm"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Imitation Learning"
    },
    "confidence": 0.9
  },
  "236": {
    "subject": {
      "name": "Imitation Learning"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "Expert Agent"
    },
    "confidence": 0.78
  },
  "237": {
    "subject": {
      "name": "Expert Demonstration"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Reinforcement Learning"
    },
    "confidence": 0.8
  },
  "238": {
    "subject": {
      "name": "Soft Q Imitation Learning"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "SQIL"
    },
    "confidence": 0.9
  },
  "239": {
    "subject": {
      "name": "SQIL"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "BC"
    },
    "confidence": 0.85
  },
  "240": {
    "subject": {
      "name": "SQIL"
    },
    "predicate": {
      "name": "achieves"
    },
    "object": {
      "name": "competitive results"
    },
    "confidence": 0.8
  },
  "241": {
    "subject": {
      "name": "SQIL"
    },
    "predicate": {
      "name": "can overcome"
    },
    "object": {
      "name": "state distribution shift problem"
    },
    "confidence": 0.75
  },
  "242": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "Bellman Equation"
    },
    "confidence": 0.85
  },
  "243": {
    "subject": {
      "name": "Soft Value Function"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Reward Function"
    },
    "confidence": 0.75
  },
  "244": {
    "subject": {
      "name": "Behavioral Cloning Method"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "Imitation Learning"
    },
    "confidence": 0.8
  },
  "245": {
    "subject": {
      "name": "Soft Q values"
    },
    "predicate": {
      "name": "correlates"
    },
    "object": {
      "name": "Reward Function"
    },
    "confidence": 0.7
  },
  "246": {
    "subject": {
      "name": "Expert Demonstration"
    },
    "predicate": {
      "name": "provides"
    },
    "object": {
      "name": "Imitation Learning"
    },
    "confidence": 0.9
  },
  "247": {
    "subject": {
      "name": "SQIL Algorithm"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "BC"
    },
    "confidence": 0.85
  },
  "248": {
    "subject": {
      "name": "SQIL Algorithm"
    },
    "predicate": {
      "name": "compares"
    },
    "object": {
      "name": "BC"
    },
    "confidence": 0.8
  },
  "249": {
    "subject": {
      "name": "SQIL Algorithm"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "GAIL"
    },
    "confidence": 0.75
  },
  "250": {
    "subject": {
      "name": "GAIL"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "BC"
    },
    "confidence": 0.7
  },
  "251": {
    "subject": {
      "name": "SQIL Algorithm"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "GAIL"
    },
    "confidence": 0.85
  },
  "252": {
    "subject": {
      "name": "GAIL"
    },
    "predicate": {
      "name": "struggles"
    },
    "object": {
      "name": "discriminator"
    },
    "confidence": 0.8
  },
  "253": {
    "subject": {
      "name": "SQIL"
    },
    "predicate": {
      "name": "benefits from"
    },
    "object": {
      "name": "constant reward"
    },
    "confidence": 0.75
  },
  "254": {
    "subject": {
      "name": "Expert Demonstration"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "SAC"
    },
    "confidence": 0.7
  },
  "255": {
    "subject": {
      "name": "SQIL"
    },
    "predicate": {
      "name": "can be adapted to"
    },
    "object": {
      "name": "continuous actions"
    },
    "confidence": 0.78
  },
  "256": {
    "subject": {
      "name": "SQIL Algorithm"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "RBC"
    },
    "confidence": 0.85
  },
  "257": {
    "subject": {
      "name": "SQIL Algorithm"
    },
    "predicate": {
      "name": "relies on"
    },
    "object": {
      "name": "imitation policy"
    },
    "confidence": 0.9
  },
  "258": {
    "subject": {
      "name": "SQIL Algorithm"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "demonstrations"
    },
    "confidence": 0.75
  },
  "259": {
    "subject": {
      "name": "SQIL Algorithm"
    },
    "predicate": {
      "name": "generalizes"
    },
    "object": {
      "name": "performance"
    },
    "confidence": 0.8
  },
  "260": {
    "subject": {
      "name": "Lunar Lander game"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "evaluation"
    },
    "confidence": 0.77
  },
  "261": {
    "subject": {
      "name": "SQIL Algorithm"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "Behavioral Cloning Method"
    },
    "confidence": 0.85
  },
  "262": {
    "subject": {
      "name": "SQIL Algorithm"
    },
    "predicate": {
      "name": "achieves"
    },
    "object": {
      "name": "competitive results"
    },
    "confidence": 0.8
  },
  "263": {
    "subject": {
      "name": "SQIL Algorithm"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Generalized Advantage Estimation"
    },
    "confidence": 0.75
  },
  "264": {
    "subject": {
      "name": "Absorbing State"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Soft Value Function"
    },
    "confidence": 0.85
  },
  "265": {
    "subject": {
      "name": "GAIL-DQL-B"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "GAIL-TRPO-B"
    },
    "confidence": 0.78
  },
  "266": {
    "subject": {
      "name": "GAIL-DQL-U"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "GAIL-TRPO-U"
    },
    "confidence": 0.78
  },
  "267": {
    "subject": {
      "name": "SQIL Algorithm"
    },
    "predicate": {
      "name": "correlates"
    },
    "object": {
      "name": "Reward Function"
    },
    "confidence": 0.8
  },
  "268": {
    "subject": {
      "name": "Demonstration Rollouts"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "Expert Demonstration"
    },
    "confidence": 0.88
  },
  "269": {
    "subject": {
      "name": "Policy Optimization from Demonstration Method"
    },
    "predicate": {
      "name": "influences"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.85
  },
  "270": {
    "subject": {
      "name": "Policy Optimization from Demonstration Method"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "Exploration Strategy"
    },
    "confidence": 0.75
  },
  "271": {
    "subject": {
      "name": "Policy Optimization from Demonstration Method"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "Sparse Reward Environment"
    },
    "confidence": 0.78
  },
  "272": {
    "subject": {
      "name": "Demonstrations"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.8
  },
  "273": {
    "subject": {
      "name": "Demonstration Data"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Policy Optimization from Demonstration Method"
    },
    "confidence": 0.7
  },
  "274": {
    "subject": {
      "name": "Policy Optimization from Demonstration Method"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "Policy Gradient Method"
    },
    "confidence": 0.85
  },
  "275": {
    "subject": {
      "name": "Policy Optimization from Demonstration Method"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "Exploration Strategy"
    },
    "confidence": 0.75
  },
  "276": {
    "subject": {
      "name": "Generative Adversarial Imitation Learning Algorithm"
    },
    "predicate": {
      "name": "works_with"
    },
    "object": {
      "name": "Expert Policy"
    },
    "confidence": 0.8
  },
  "277": {
    "subject": {
      "name": "Imitation Learning"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Reward Shaping Method"
    },
    "confidence": 0.7
  },
  "278": {
    "subject": {
      "name": "Reward Redistribution Method"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "Sparse Reward Environment"
    },
    "confidence": 0.78
  },
  "279": {
    "subject": {
      "name": "Generative Adversarial Imitation Learning"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "policy"
    },
    "confidence": 0.85
  },
  "280": {
    "subject": {
      "name": "GAIL"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Imitation Learning"
    },
    "confidence": 0.75
  },
  "281": {
    "subject": {
      "name": "Reward Function"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.8
  },
  "282": {
    "subject": {
      "name": "MDP"
    },
    "predicate": {
      "name": "defines"
    },
    "object": {
      "name": "state space"
    },
    "confidence": 0.9
  },
  "283": {
    "subject": {
      "name": "State Action Pair"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "Policy"
    },
    "confidence": 0.7
  },
  "284": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Policy Optimization with Demonstrations"
    },
    "confidence": 0.85
  },
  "285": {
    "subject": {
      "name": "Expert Policy"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "Agent"
    },
    "confidence": 0.8
  },
  "286": {
    "subject": {
      "name": "Sparse Reward Environment"
    },
    "predicate": {
      "name": "poses challenges for"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.7
  },
  "287": {
    "subject": {
      "name": "Expert Demonstration"
    },
    "predicate": {
      "name": "leverages"
    },
    "object": {
      "name": "Policy Optimization from Demonstration Method"
    },
    "confidence": 0.75
  },
  "288": {
    "subject": {
      "name": "Demonstrated Trajectories"
    },
    "predicate": {
      "name": "inspire"
    },
    "object": {
      "name": "Agent"
    },
    "confidence": 0.8
  },
  "289": {
    "subject": {
      "name": "Policy"
    },
    "predicate": {
      "name": "defines"
    },
    "object": {
      "name": "Learning Objective"
    },
    "confidence": 0.85
  },
  "290": {
    "subject": {
      "name": "Occupancy Measure"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "Optimization Process"
    },
    "confidence": 0.8
  },
  "291": {
    "subject": {
      "name": "Adversarial Training Method"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "Policy"
    },
    "confidence": 0.9
  },
  "292": {
    "subject": {
      "name": "Policy"
    },
    "predicate": {
      "name": "integrates"
    },
    "object": {
      "name": "Expert Demonstration"
    },
    "confidence": 0.75
  },
  "293": {
    "subject": {
      "name": "Jensen-Shannon Divergence"
    },
    "predicate": {
      "name": "compares"
    },
    "object": {
      "name": "Policy"
    },
    "confidence": 0.7
  },
  "294": {
    "subject": {
      "name": "Generative Adversarial Network"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Policy Gradient Method"
    },
    "confidence": 0.85
  },
  "295": {
    "subject": {
      "name": "Discriminator Model"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "Generative Adversarial Network"
    },
    "confidence": 0.78
  },
  "296": {
    "subject": {
      "name": "Expert Policy"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "Generative Adversarial Network"
    },
    "confidence": 0.9
  },
  "297": {
    "subject": {
      "name": "Expert Demonstration"
    },
    "predicate": {
      "name": "aids"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.8
  },
  "298": {
    "subject": {
      "name": "Deep Q-Network"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Q-learning Method"
    },
    "confidence": 0.85
  },
  "299": {
    "subject": {
      "name": "Deep Q-Network"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "Reward Function"
    },
    "confidence": 0.9
  },
  "300": {
    "subject": {
      "name": "Expert Demonstration"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "Policy Optimization from Demonstration Method"
    },
    "confidence": 0.88
  },
  "301": {
    "subject": {
      "name": "Replay Memory"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "Experience Replay Buffer"
    },
    "confidence": 0.8
  },
  "302": {
    "subject": {
      "name": "POfD"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "GAIL"
    },
    "confidence": 0.85
  },
  "303": {
    "subject": {
      "name": "POfD"
    },
    "predicate": {
      "name": "achieves"
    },
    "object": {
      "name": "expert-level performance"
    },
    "confidence": 0.9
  },
  "304": {
    "subject": {
      "name": "TRPO"
    },
    "predicate": {
      "name": "fails to explore"
    },
    "object": {
      "name": "sparse environments"
    },
    "confidence": 0.95
  },
  "305": {
    "subject": {
      "name": "DQfD"
    },
    "predicate": {
      "name": "converges to"
    },
    "object": {
      "name": "imperfect demonstration data"
    },
    "confidence": 0.88
  },
  "306": {
    "subject": {
      "name": "Humanoid-v1"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.8
  },
  "307": {
    "subject": {
      "name": "Reacher-v1"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Sparse Reward Environment"
    },
    "confidence": 0.75
  },
  "308": {
    "subject": {
      "name": "POfD"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "Learning from Demonstration Method"
    },
    "confidence": 0.9
  },
  "309": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Markov Decision Process"
    },
    "confidence": 0.85
  },
  "310": {
    "subject": {
      "name": "Dataset"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.9
  },
  "311": {
    "subject": {
      "name": "Minecraft"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "Video Game Environment"
    },
    "confidence": 0.8
  },
  "312": {
    "subject": {
      "name": "Action Primitive"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Craftable Item"
    },
    "confidence": 0.75
  },
  "313": {
    "subject": {
      "name": "MineRL"
    },
    "predicate": {
      "name": "promotes"
    },
    "object": {
      "name": "Imitation Learning"
    },
    "confidence": 0.88
  },
  "314": {
    "subject": {
      "name": "Minecraft"
    },
    "predicate": {
      "name": "is a"
    },
    "object": {
      "name": "Video Game Environment"
    },
    "confidence": 0.95
  },
  "315": {
    "subject": {
      "name": "Minecraft"
    },
    "predicate": {
      "name": "involves"
    },
    "object": {
      "name": "Exploration Strategy"
    },
    "confidence": 0.9
  },
  "316": {
    "subject": {
      "name": "Minecraft"
    },
    "predicate": {
      "name": "includes"
    },
    "object": {
      "name": "Action Primitive"
    },
    "confidence": 0.85
  },
  "317": {
    "subject": {
      "name": "human demonstrations"
    },
    "predicate": {
      "name": "recorded during"
    },
    "object": {
      "name": "Minecraft"
    },
    "confidence": 0.8
  },
  "318": {
    "subject": {
      "name": "MineRL"
    },
    "predicate": {
      "name": "provides"
    },
    "object": {
      "name": "dataset"
    },
    "confidence": 0.92
  },
  "319": {
    "subject": {
      "name": "Wood"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Tools"
    },
    "confidence": 0.85
  },
  "320": {
    "subject": {
      "name": "Iron Pickaxe"
    },
    "predicate": {
      "name": "obtains"
    },
    "object": {
      "name": "Key materials"
    },
    "confidence": 0.75
  },
  "321": {
    "subject": {
      "name": "Diamonds"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "High-level Minecraft play"
    },
    "confidence": 0.8
  },
  "322": {
    "subject": {
      "name": "Cooked meat"
    },
    "predicate": {
      "name": "is used to"
    },
    "object": {
      "name": "replenish stamina"
    },
    "confidence": 0.78
  },
  "323": {
    "subject": {
      "name": "Bed"
    },
    "predicate": {
      "name": "is required for"
    },
    "object": {
      "name": "Sleeping"
    },
    "confidence": 0.82
  },
  "324": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Hierarchical Reinforcement Learning Architecture"
    },
    "confidence": 0.8
  },
  "325": {
    "subject": {
      "name": "Obtain<Item>"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "ObtainIronPickaxe"
    },
    "confidence": 0.75
  },
  "326": {
    "subject": {
      "name": "ObtainDiamond"
    },
    "predicate": {
      "name": "overlaps with"
    },
    "object": {
      "name": "ObtainIronPickaxe"
    },
    "confidence": 0.7
  },
  "327": {
    "subject": {
      "name": "ObtainIronPickaxe"
    },
    "predicate": {
      "name": "requires"
    },
    "object": {
      "name": "iron ore"
    },
    "confidence": 0.85
  },
  "328": {
    "subject": {
      "name": "ObtainCookedMeat"
    },
    "predicate": {
      "name": "does not require"
    },
    "object": {
      "name": "iron ore"
    },
    "confidence": 0.8
  },
  "329": {
    "subject": {
      "name": "Agent"
    },
    "predicate": {
      "name": "uses"
    },
    "object": {
      "name": "State Association Learning Method"
    },
    "confidence": 0.9
  },
  "330": {
    "subject": {
      "name": "State Association Learning Method"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "Synthetic Return"
    },
    "confidence": 0.85
  },
  "331": {
    "subject": {
      "name": "Synthetic Return"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "Policy"
    },
    "confidence": 0.8
  },
  "332": {
    "subject": {
      "name": "Agent"
    },
    "predicate": {
      "name": "augments"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.8
  },
  "333": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "solves"
    },
    "object": {
      "name": "Atari Skiing"
    },
    "confidence": 0.75
  },
  "334": {
    "subject": {
      "name": "Agent"
    },
    "predicate": {
      "name": "learns"
    },
    "object": {
      "name": "State Representation"
    },
    "confidence": 0.9
  },
  "335": {
    "subject": {
      "name": "State Representation"
    },
    "predicate": {
      "name": "predicts"
    },
    "object": {
      "name": "Reward"
    },
    "confidence": 0.9
  },
  "336": {
    "subject": {
      "name": "State Association Learning Method"
    },
    "predicate": {
      "name": "assigns_credit_to"
    },
    "object": {
      "name": "State"
    },
    "confidence": 0.8
  },
  "337": {
    "subject": {
      "name": "Agent"
    },
    "predicate": {
      "name": "assigns_credit_to"
    },
    "object": {
      "name": "key-pickup"
    },
    "confidence": 0.85
  },
  "338": {
    "subject": {
      "name": "SA-learning"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "long-term credit assignment"
    },
    "confidence": 0.9
  },
  "339": {
    "subject": {
      "name": "opened door"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "receive reward"
    },
    "confidence": 0.8
  },
  "340": {
    "subject": {
      "name": "buffer of state representations"
    },
    "predicate": {
      "name": "aids"
    },
    "object": {
      "name": "predict future rewards"
    },
    "confidence": 0.75
  },
  "341": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "influences"
    },
    "object": {
      "name": "Synthetic Return"
    },
    "confidence": 0.85
  },
  "342": {
    "subject": {
      "name": "Agent"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "Reward"
    },
    "confidence": 0.9
  },
  "343": {
    "subject": {
      "name": "Agent"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "Delayed Reward Mechanism"
    },
    "confidence": 0.8
  },
  "344": {
    "subject": {
      "name": "Agent"
    },
    "predicate": {
      "name": "correlates"
    },
    "object": {
      "name": "Trigger State"
    },
    "confidence": 0.75
  },
  "345": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "utilizes"
    },
    "object": {
      "name": "Reward Function"
    },
    "confidence": 0.8
  },
  "346": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Markov Decision Process"
    },
    "confidence": 0.85
  },
  "347": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "State Association Learning Method"
    },
    "confidence": 0.7
  },
  "348": {
    "subject": {
      "name": "Sequence-Markov Decision Process"
    },
    "predicate": {
      "name": "complements"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.75
  },
  "349": {
    "subject": {
      "name": "Example-Based Control"
    },
    "predicate": {
      "name": "promotes"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.85
  },
  "350": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Markov Decision Process"
    },
    "confidence": 0.75
  },
  "351": {
    "subject": {
      "name": "Example-Based Control"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "Reward Function"
    },
    "confidence": 0.8
  },
  "352": {
    "subject": {
      "name": "Control Strategy"
    },
    "predicate": {
      "name": "integrates"
    },
    "object": {
      "name": "Recursive Classification"
    },
    "confidence": 0.7
  },
  "353": {
    "subject": {
      "name": "Example-Based Policy Search"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "Success Example"
    },
    "confidence": 0.9
  },
  "354": {
    "subject": {
      "name": "Reward Function"
    },
    "predicate": {
      "name": "replaces"
    },
    "object": {
      "name": "Success Example"
    },
    "confidence": 0.85
  },
  "355": {
    "subject": {
      "name": "Robust Example-Based Control"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "Bellman Equation"
    },
    "confidence": 0.75
  },
  "356": {
    "subject": {
      "name": "Success Example"
    },
    "predicate": {
      "name": "converges_to"
    },
    "object": {
      "name": "Value Function"
    },
    "confidence": 0.8
  },
  "357": {
    "subject": {
      "name": "Policy"
    },
    "predicate": {
      "name": "maximizes"
    },
    "object": {
      "name": "Probability"
    },
    "confidence": 0.78
  },
  "358": {
    "subject": {
      "name": "example-based control"
    },
    "predicate": {
      "name": "studies"
    },
    "object": {
      "name": "Reward Function"
    },
    "confidence": 0.7
  },
  "359": {
    "subject": {
      "name": "Algorithm"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "two-stage approaches"
    },
    "confidence": 0.88
  },
  "360": {
    "subject": {
      "name": "Example-Based Control"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Goal"
    },
    "confidence": 0.85
  },
  "361": {
    "subject": {
      "name": "ValueDICE"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "KL Loss Function"
    },
    "confidence": 0.9
  },
  "362": {
    "subject": {
      "name": "SQIL Algorithm"
    },
    "predicate": {
      "name": "tracks"
    },
    "object": {
      "name": "Imitation Learning"
    },
    "confidence": 0.78
  },
  "363": {
    "subject": {
      "name": "C-learning"
    },
    "predicate": {
      "name": "influences"
    },
    "object": {
      "name": "Example-Based Control"
    },
    "confidence": 0.8
  },
  "364": {
    "subject": {
      "name": "example_string"
    },
    "predicate": {
      "name": "generalizes"
    },
    "object": {
      "name": "imitation learning method"
    },
    "confidence": 0.75
  },
  "365": {
    "subject": {
      "name": "Example-Based Control"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.8
  },
  "366": {
    "subject": {
      "name": "Success Example"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "Policy"
    },
    "confidence": 0.7
  },
  "367": {
    "subject": {
      "name": "Transition Distribution"
    },
    "predicate": {
      "name": "provides"
    },
    "object": {
      "name": "Data Collection Methodology"
    },
    "confidence": 0.75
  },
  "368": {
    "subject": {
      "name": "Future Success Classifier"
    },
    "predicate": {
      "name": "predicts"
    },
    "object": {
      "name": "Success Example"
    },
    "confidence": 0.85
  },
  "369": {
    "subject": {
      "name": "Future Success Classifier"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Example-Based Control"
    },
    "confidence": 0.9
  },
  "370": {
    "subject": {
      "name": "Recursive Classification"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Future Success Classifier"
    },
    "confidence": 0.75
  },
  "371": {
    "subject": {
      "name": "Bayes-optimal Classifier"
    },
    "predicate": {
      "name": "integrates"
    },
    "object": {
      "name": "Future Success Classifier"
    },
    "confidence": 0.8
  },
  "372": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Robust Example-Based Control"
    },
    "confidence": 0.85
  },
  "373": {
    "subject": {
      "name": "Success Example"
    },
    "predicate": {
      "name": "correlates"
    },
    "object": {
      "name": "Bayes-optimal Classifier"
    },
    "confidence": 0.75
  },
  "374": {
    "subject": {
      "name": "Reward Function"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "Policy"
    },
    "confidence": 0.78
  },
  "375": {
    "subject": {
      "name": "Tabular Model"
    },
    "predicate": {
      "name": "converges_to"
    },
    "object": {
      "name": "Value Iteration Algorithm"
    },
    "confidence": 0.8
  },
  "376": {
    "subject": {
      "name": "Policy"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "Performance Metric"
    },
    "confidence": 0.9
  },
  "377": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "Bellman Equation"
    },
    "confidence": 0.85
  },
  "378": {
    "subject": {
      "name": "Reward Function"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "Policy"
    },
    "confidence": 0.8
  },
  "379": {
    "subject": {
      "name": "Imitation Learning"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "policy"
    },
    "confidence": 0.75
  },
  "380": {
    "subject": {
      "name": "policy"
    },
    "predicate": {
      "name": "maximizes"
    },
    "object": {
      "name": "Probability of Success"
    },
    "confidence": 0.9
  },
  "381": {
    "subject": {
      "name": "data-driven approach"
    },
    "predicate": {
      "name": "complements"
    },
    "object": {
      "name": "reinforcement learning"
    },
    "confidence": 0.78
  },
  "382": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "utilizes"
    },
    "object": {
      "name": "Bellman Equation"
    },
    "confidence": 0.85
  },
  "383": {
    "subject": {
      "name": "Markov Decision Process"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Reinforcement Learning Algorithm"
    },
    "confidence": 0.9
  },
  "384": {
    "subject": {
      "name": "Value Iteration Algorithm"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Temporal Difference Method"
    },
    "confidence": 0.8
  },
  "385": {
    "subject": {
      "name": "Temporal Difference Method"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "Value Iteration Algorithm"
    },
    "confidence": 0.75
  },
  "386": {
    "subject": {
      "name": "Bellman Equation"
    },
    "predicate": {
      "name": "provides"
    },
    "object": {
      "name": "expected value of rewards"
    },
    "confidence": 0.95
  },
  "387": {
    "subject": {
      "name": "policy"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "iterated RCE"
    },
    "confidence": 0.85
  },
  "388": {
    "subject": {
      "name": "success examples"
    },
    "predicate": {
      "name": "correlates"
    },
    "object": {
      "name": "objective function"
    },
    "confidence": 0.75
  },
  "389": {
    "subject": {
      "name": "learning a reward function"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "density model"
    },
    "confidence": 0.7
  },
  "390": {
    "subject": {
      "name": "example-based control"
    },
    "predicate": {
      "name": "depends on"
    },
    "object": {
      "name": "auxiliary function approximators"
    },
    "confidence": 0.8
  },
  "391": {
    "subject": {
      "name": "n-step returns"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "performance"
    },
    "confidence": 0.85
  },
  "392": {
    "subject": {
      "name": "current policy"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "historical average policy"
    },
    "confidence": 0.8
  },
  "393": {
    "subject": {
      "name": "n-step returns"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "performance on sawyer_push and sawyer_drawer_open tasks"
    },
    "confidence": 0.9
  },
  "394": {
    "subject": {
      "name": "RCE"
    },
    "predicate": {
      "name": "works at least as well as"
    },
    "object": {
      "name": "behavior policy"
    },
    "confidence": 0.75
  },
  "395": {
    "subject": {
      "name": "RCE"
    },
    "predicate": {
      "name": "regularizes"
    },
    "object": {
      "name": "implicit reward"
    },
    "confidence": 0.7
  },
  "396": {
    "subject": {
      "name": "Agent-Temporal Attention for Reward Redistribution in Episodic Multi-Agent Reinforcement Learning"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "multi-agent credit assignment problem"
    },
    "confidence": 0.9
  },
  "397": {
    "subject": {
      "name": "AREL"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "higher rewards"
    },
    "confidence": 0.85
  },
  "398": {
    "subject": {
      "name": "AREL"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "win rates"
    },
    "confidence": 0.85
  },
  "399": {
    "subject": {
      "name": "AREL"
    },
    "predicate": {
      "name": "integrates"
    },
    "object": {
      "name": "any given MARL algorithm"
    },
    "confidence": 0.8
  },
  "400": {
    "subject": {
      "name": "multi-agent reinforcement learning (MARL)"
    },
    "predicate": {
      "name": "involves"
    },
    "object": {
      "name": "multiple autonomous agents"
    },
    "confidence": 0.95
  },
  "401": {
    "subject": {
      "name": "AREL"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "reward signal"
    },
    "confidence": 0.75
  },
  "402": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "Temporal Attention Module"
    },
    "confidence": 0.85
  },
  "403": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "Reward Redistribution Method"
    },
    "confidence": 0.88
  },
  "404": {
    "subject": {
      "name": "Agent"
    },
    "predicate": {
      "name": "collaborates with"
    },
    "object": {
      "name": "Agent"
    },
    "confidence": 0.75
  },
  "405": {
    "subject": {
      "name": "Attention Mechanism"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Agent-Temporal Attention Block"
    },
    "confidence": 0.9
  },
  "406": {
    "subject": {
      "name": "Temporal Attention Module"
    },
    "predicate": {
      "name": "modulates"
    },
    "object": {
      "name": "Episodic Reward"
    },
    "confidence": 0.82
  },
  "407": {
    "subject": {
      "name": "Multi-Agent Reinforcement Learning"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Reward Redistribution Method"
    },
    "confidence": 0.8
  },
  "408": {
    "subject": {
      "name": "Attention Mechanism"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "multi-agent credit assignment"
    },
    "confidence": 0.85
  },
  "409": {
    "subject": {
      "name": "multi-agent credit assignment"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "delayed rewards"
    },
    "confidence": 0.8
  },
  "410": {
    "subject": {
      "name": "AREL"
    },
    "predicate": {
      "name": "overcomes"
    },
    "object": {
      "name": "scalability challenges"
    },
    "confidence": 0.9
  },
  "411": {
    "subject": {
      "name": "Value decomposition networks"
    },
    "predicate": {
      "name": "decomposes"
    },
    "object": {
      "name": "centralized value"
    },
    "confidence": 0.75
  },
  "412": {
    "subject": {
      "name": "QTRAN"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "credit assignment"
    },
    "confidence": 0.7
  },
  "413": {
    "subject": {
      "name": "Centralized Training with Decentralized Execution Paradigm"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Reward Redistribution Method"
    },
    "confidence": 0.85
  },
  "414": {
    "subject": {
      "name": "Reward Redistribution Method"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "Temporal Attention Module"
    },
    "confidence": 0.8
  },
  "415": {
    "subject": {
      "name": "Agent-Temporal Attention Block"
    },
    "predicate": {
      "name": "influences"
    },
    "object": {
      "name": "Decentralized Policies"
    },
    "confidence": 0.75
  },
  "416": {
    "subject": {
      "name": "Decentralized Policies"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "Effective Multi-Agent Temporal Credit Assignment"
    },
    "confidence": 0.9
  },
  "417": {
    "subject": {
      "name": "Temporal Attention Module"
    },
    "predicate": {
      "name": "modulates"
    },
    "object": {
      "name": "Agent Attention Module"
    },
    "confidence": 0.78
  },
  "418": {
    "subject": {
      "name": "Agent"
    },
    "predicate": {
      "name": "shares"
    },
    "object": {
      "name": "Agent-Group Embedding"
    },
    "confidence": 0.85
  },
  "419": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Identification of Agents"
    },
    "confidence": 0.8
  },
  "420": {
    "subject": {
      "name": "Reward Redistribution Method"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "Sparse Rewards"
    },
    "confidence": 0.75
  },
  "421": {
    "subject": {
      "name": "Temporal Decomposition of Reward"
    },
    "predicate": {
      "name": "assesses"
    },
    "object": {
      "name": "Contributions of Agents"
    },
    "confidence": 0.78
  },
  "422": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "Multi-Agent System"
    },
    "confidence": 0.82
  },
  "423": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "transforms"
    },
    "object": {
      "name": "Reward Redistribution Method"
    },
    "confidence": 0.85
  },
  "424": {
    "subject": {
      "name": "Reward Redistribution Method"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Optimal Reward Redistribution"
    },
    "confidence": 0.78
  },
  "425": {
    "subject": {
      "name": "Episode"
    },
    "predicate": {
      "name": "correlates"
    },
    "object": {
      "name": "State Space"
    },
    "confidence": 0.8
  },
  "426": {
    "subject": {
      "name": "Optimal Reward Redistribution"
    },
    "predicate": {
      "name": "integrates"
    },
    "object": {
      "name": "Bellman Equation"
    },
    "confidence": 0.75
  },
  "427": {
    "subject": {
      "name": "Decentralized Partially Observable Sequence-Markov Decision Process"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Decentralized Partially Observable Sequence-Markov Decision Process"
    },
    "confidence": 1.0
  },
  "428": {
    "subject": {
      "name": "Decentralized Partially Observable Sequence-Markov Decision Process"
    },
    "predicate": {
      "name": "predicts"
    },
    "object": {
      "name": "Optimal Policy"
    },
    "confidence": 0.8
  },
  "429": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Performance Metric"
    },
    "confidence": 0.85
  },
  "430": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "guides"
    },
    "object": {
      "name": "Task"
    },
    "confidence": 0.78
  },
  "431": {
    "subject": {
      "name": "Attention Mechanism"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "Reward Function"
    },
    "confidence": 0.9
  },
  "432": {
    "subject": {
      "name": "RUDDER"
    },
    "predicate": {
      "name": "outperforms"
    },
    "object": {
      "name": "Sequence Modeling"
    },
    "confidence": 0.75
  },
  "433": {
    "subject": {
      "name": "Agent"
    },
    "predicate": {
      "name": "interacts with"
    },
    "object": {
      "name": "Environment"
    },
    "confidence": 0.88
  },
  "434": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "integrates"
    },
    "object": {
      "name": "Reward Redistribution Method"
    },
    "confidence": 0.85
  },
  "435": {
    "subject": {
      "name": "Reward Redistribution Method"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "episodic reward"
    },
    "confidence": 0.78
  },
  "436": {
    "subject": {
      "name": "MARL algorithm"
    },
    "predicate": {
      "name": "learns"
    },
    "object": {
      "name": "Value Function"
    },
    "confidence": 0.8
  },
  "437": {
    "subject": {
      "name": "AREL"
    },
    "predicate": {
      "name": "provides"
    },
    "object": {
      "name": "input"
    },
    "confidence": 0.82
  },
  "438": {
    "subject": {
      "name": "Attention Mechanism"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "effective credit assignment"
    },
    "confidence": 0.75
  },
  "439": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "Attention Mechanism"
    },
    "confidence": 0.8
  },
  "440": {
    "subject": {
      "name": "Credit Assignment Algorithm"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": " redistributed rewards"
    },
    "confidence": 0.75
  },
  "441": {
    "subject": {
      "name": "Transformer Model"
    },
    "predicate": {
      "name": "applies"
    },
    "object": {
      "name": "multi-head attention"
    },
    "confidence": 0.85
  },
  "442": {
    "subject": {
      "name": "Agent"
    },
    "predicate": {
      "name": "experiments with"
    },
    "object": {
      "name": "Particle World"
    },
    "confidence": 0.7
  },
  "443": {
    "subject": {
      "name": "Agent"
    },
    "predicate": {
      "name": "competes in"
    },
    "object": {
      "name": "StarCraft"
    },
    "confidence": 0.7
  },
  "444": {
    "subject": {
      "name": "Attention Mechanism"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "average rewards"
    },
    "confidence": 0.85
  },
  "445": {
    "subject": {
      "name": "QMIX"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "base algorithm"
    },
    "confidence": 0.8
  },
  "446": {
    "subject": {
      "name": "QMIX"
    },
    "predicate": {
      "name": "utilizes"
    },
    "object": {
      "name": "deep recurrent Q-network"
    },
    "confidence": 0.9
  },
  "447": {
    "subject": {
      "name": "QMIX"
    },
    "predicate": {
      "name": "trains"
    },
    "object": {
      "name": "RMSprop"
    },
    "confidence": 0.75
  },
  "448": {
    "subject": {
      "name": "AREL"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "energy consumption"
    },
    "confidence": 0.7
  },
  "449": {
    "subject": {
      "name": "experience buffer"
    },
    "predicate": {
      "name": "fills"
    },
    "object": {
      "name": "trajectories"
    },
    "confidence": 0.78
  },
  "450": {
    "subject": {
      "name": "trajectory"
    },
    "predicate": {
      "name": "is part of"
    },
    "object": {
      "name": "training episodes"
    },
    "confidence": 0.82
  },
  "451": {
    "subject": {
      "name": "Hierarchical Reinforcement Learning Architecture"
    },
    "predicate": {
      "name": "enables"
    },
    "object": {
      "name": "goal-directed behavior"
    },
    "confidence": 0.85
  },
  "452": {
    "subject": {
      "name": "Intrinsic Motivation"
    },
    "predicate": {
      "name": "improves"
    },
    "object": {
      "name": "exploration strategy"
    },
    "confidence": 0.75
  },
  "453": {
    "subject": {
      "name": "Hierarchical Deep Reinforcement Learning"
    },
    "predicate": {
      "name": "integrates"
    },
    "object": {
      "name": "goal-driven intrinsically motivated deep reinforcement learning"
    },
    "confidence": 0.9
  },
  "454": {
    "subject": {
      "name": "h-DQN"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "effective exploration in sparse environments"
    },
    "confidence": 0.8
  },
  "455": {
    "subject": {
      "name": "Hierarchical action-value functions"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "sparse feedback challenge"
    },
    "confidence": 0.7
  },
  "456": {
    "subject": {
      "name": "Meta Controller"
    },
    "predicate": {
      "name": "estimates"
    },
    "object": {
      "name": "action-value function"
    },
    "confidence": 0.85
  },
  "457": {
    "subject": {
      "name": "Controller"
    },
    "predicate": {
      "name": "produces"
    },
    "object": {
      "name": "policy over actions"
    },
    "confidence": 0.88
  },
  "458": {
    "subject": {
      "name": "Controller"
    },
    "predicate": {
      "name": "takes in"
    },
    "object": {
      "name": "states"
    },
    "confidence": 0.9
  },
  "459": {
    "subject": {
      "name": "Meta Controller"
    },
    "predicate": {
      "name": "looks at"
    },
    "object": {
      "name": "raw states"
    },
    "confidence": 0.86
  },
  "460": {
    "subject": {
      "name": "Internal Critic"
    },
    "predicate": {
      "name": "provides"
    },
    "object": {
      "name": "positive reward"
    },
    "confidence": 0.87
  },
  "461": {
    "subject": {
      "name": "Reward Function"
    },
    "predicate": {
      "name": "maximizes"
    },
    "object": {
      "name": "cumulative intrinsic reward"
    },
    "confidence": 0.84
  },
  "462": {
    "subject": {
      "name": "Meta Controller"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "cumulative extrinsic reward"
    },
    "confidence": 0.89
  },
  "463": {
    "subject": {
      "name": "Reward Redistribution Method"
    },
    "predicate": {
      "name": "addresses"
    },
    "object": {
      "name": "external reward"
    },
    "confidence": 0.81
  },
  "464": {
    "subject": {
      "name": "Reinforcement Learning Algorithm"
    },
    "predicate": {
      "name": "integrates"
    },
    "object": {
      "name": "Q-learning Method"
    },
    "confidence": 0.85
  },
  "465": {
    "subject": {
      "name": "Markov Decision Process"
    },
    "predicate": {
      "name": "is equivalent to"
    },
    "object": {
      "name": "Sequence-Markov Decision Process"
    },
    "confidence": 0.8
  },
  "466": {
    "subject": {
      "name": "Reward Redistribution Method"
    },
    "predicate": {
      "name": "optimizes"
    },
    "object": {
      "name": "Reward Function"
    },
    "confidence": 0.7
  },
  "467": {
    "subject": {
      "name": "State"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "Optimal Reward Redistribution"
    },
    "confidence": 0.75
  },
  "468": {
    "subject": {
      "name": "Agent"
    },
    "predicate": {
      "name": "yields"
    },
    "object": {
      "name": "Intrinsic Reward"
    },
    "confidence": 0.9
  },
  "469": {
    "subject": {
      "name": "Meta Controller"
    },
    "predicate": {
      "name": "controls"
    },
    "object": {
      "name": "Controller"
    },
    "confidence": 0.95
  },
  "470": {
    "subject": {
      "name": "Controller"
    },
    "predicate": {
      "name": "satisfies"
    },
    "object": {
      "name": "goal"
    },
    "confidence": 0.9
  },
  "471": {
    "subject": {
      "name": "goal"
    },
    "predicate": {
      "name": "is prerequisite for"
    },
    "object": {
      "name": "reward"
    },
    "confidence": 0.85
  },
  "472": {
    "subject": {
      "name": "experience replay memoriesD1"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "106"
    },
    "confidence": 0.8
  },
  "473": {
    "subject": {
      "name": "experience replay memoriesD2"
    },
    "predicate": {
      "name": "is fixed point of"
    },
    "object": {
      "name": "5 \u00b7104"
    },
    "confidence": 0.8
  }
}