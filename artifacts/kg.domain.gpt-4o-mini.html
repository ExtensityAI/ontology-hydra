<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 750px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             
             #loadingBar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width: 100%;
                 height: 750px;
                 background-color:rgba(200,200,200,0.8);
                 -webkit-transition: all 0.5s ease;
                 -moz-transition: all 0.5s ease;
                 -ms-transition: all 0.5s ease;
                 -o-transition: all 0.5s ease;
                 transition: all 0.5s ease;
                 opacity:1;
             }

             #bar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width:20px;
                 height:20px;
                 margin:auto auto auto auto;
                 border-radius:11px;
                 border:2px solid rgba(30,30,30,0.05);
                 background: rgb(0, 173, 246); /* Old browsers */
                 box-shadow: 2px 0px 4px rgba(0,0,0,0.4);
             }

             #border {
                 position:absolute;
                 top:10px;
                 left:10px;
                 width:500px;
                 height:23px;
                 margin:auto auto auto auto;
                 box-shadow: 0px 0px 4px rgba(0,0,0,0.2);
                 border-radius:10px;
             }

             #text {
                 position:absolute;
                 top:8px;
                 left:530px;
                 width:30px;
                 height:50px;
                 margin:auto auto auto auto;
                 font-size:22px;
                 color: #000000;
             }

             div.outerBorder {
                 position:relative;
                 top:400px;
                 width:600px;
                 height:44px;
                 margin:auto auto auto auto;
                 border:8px solid rgba(0,0,0,0.1);
                 background: rgb(252,252,252); /* Old browsers */
                 background: -moz-linear-gradient(top,  rgba(252,252,252,1) 0%, rgba(237,237,237,1) 100%); /* FF3.6+ */
                 background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,rgba(252,252,252,1)), color-stop(100%,rgba(237,237,237,1))); /* Chrome,Safari4+ */
                 background: -webkit-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Chrome10+,Safari5.1+ */
                 background: -o-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Opera 11.10+ */
                 background: -ms-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* IE10+ */
                 background: linear-gradient(to bottom,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* W3C */
                 filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#fcfcfc', endColorstr='#ededed',GradientType=0 ); /* IE6-9 */
                 border-radius:72px;
                 box-shadow: 0px 0px 10px rgba(0,0,0,0.2);
             }
             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
            <div id="loadingBar">
              <div class="outerBorder">
                <div id="text">0%</div>
                <div id="border">
                  <div id="bar"></div>
                </div>
              </div>
            </div>
        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#F1C40F", "font": {"color": "black"}, "id": "Reinforcement Learning Algorithm", "label": "Reinforcement Learning Algorithm", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Return Decomposition Technique", "label": "Return Decomposition Technique", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Markov Decision Process", "label": "Markov Decision Process", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Optimal Reward Redistribution", "label": "Optimal Reward Redistribution", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Reward Redistribution Method", "label": "Reward Redistribution Method", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Sequence-Markov Decision Process", "label": "Sequence-Markov Decision Process", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Contribution Analysis", "label": "Contribution Analysis", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Delayed Reward Mechanism", "label": "Delayed Reward Mechanism", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Q-learning Method", "label": "Q-learning Method", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Advantage Function", "label": "Advantage Function", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Temporal Value Transport Mechanism", "label": "Temporal Value Transport Mechanism", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "expected future rewards", "label": "expected future rewards", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Bellman Equation", "label": "Bellman Equation", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Q-value estimation", "label": "Q-value estimation", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Long Short-Term Memory Network", "label": "Long Short-Term Memory Network", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "RUDDER", "label": "RUDDER", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Atari games", "label": "Atari games", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Value Function", "label": "Value Function", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Human Expert Episodes", "label": "Human Expert Episodes", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Q(\u03bb)", "label": "Q(\u03bb)", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "MC", "label": "MC", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "MCTS", "label": "MCTS", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Supplements Theorem S8", "label": "Supplements Theorem S8", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "PPO baseline", "label": "PPO baseline", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "TD(\u03bb)", "label": "TD(\u03bb)", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "reward shaping methods", "label": "reward shaping methods", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "scores", "label": "scores", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "learning time", "label": "learning time", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "reward redistribution", "label": "reward redistribution", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Feudal Network", "label": "Feudal Network", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Hierarchical Reinforcement Learning", "label": "Hierarchical Reinforcement Learning", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Manager Module", "label": "Manager Module", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "lower temporal resolution", "label": "lower temporal resolution", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Worker Module", "label": "Worker Module", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "primitive actions", "label": "primitive actions", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "temporal sub-policies", "label": "temporal sub-policies", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "abstract goals", "label": "abstract goals", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "goals", "label": "goals", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Goal", "label": "Goal", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Dilated LSTM", "label": "Dilated LSTM", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Transition Policy", "label": "Transition Policy", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Action Primitive", "label": "Action Primitive", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Recurrent Neural Network", "label": "Recurrent Neural Network", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Task", "label": "Task", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "FuN", "label": "FuN", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Goal Embedding", "label": "Goal Embedding", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "actions", "label": "actions", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "policy", "label": "policy", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "goal vector", "label": "goal vector", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "LSTM", "label": "LSTM", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "T-maze+", "label": "T-maze+", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Non-match", "label": "Non-match", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Ablative analysis", "label": "Ablative analysis", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "transition policy gradient for training the Manager", "label": "transition policy gradient for training the Manager", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "memory for water maze task", "label": "memory for water maze task", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Gaussian Distribution", "label": "Gaussian Distribution", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Policy Gradient Method", "label": "Policy Gradient Method", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Intrinsic Reward", "label": "Intrinsic Reward", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Primitive Actions", "label": "Primitive Actions", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Behavioural Primitive", "label": "Behavioural Primitive", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Unsupervised visual representation learning", "label": "Unsupervised visual representation learning", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "the challenge of computationally intensive training", "label": "the challenge of computationally intensive training", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "better methods for exploring pretraining algorithms", "label": "better methods for exploring pretraining algorithms", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Evaluation Protocol", "label": "Evaluation Protocol", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "existing self-supervised learning recipes", "label": "existing self-supervised learning recipes", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Linear Probing Task", "label": "Linear Probing Task", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "actual downstream control performance", "label": "actual downstream control performance", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Reward Probing Method", "label": "Reward Probing Method", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Unsupervised Learning Method", "label": "Unsupervised Learning Method", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Reward Predicting Method", "label": "Reward Predicting Method", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Value Prediction", "label": "Value Prediction", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Reward Prediction", "label": "Reward Prediction", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Value-Based Control", "label": "Value-Based Control", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Critic in Actor-Critic Methods", "label": "Critic in Actor-Critic Methods", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Data Collection Methodology", "label": "Data Collection Methodology", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Action Prediction", "label": "Action Prediction", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Transition Model", "label": "Transition Model", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Representation Learning", "label": "Representation Learning", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "probing performance", "label": "probing performance", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Barlow", "label": "Barlow", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "probing scores", "label": "probing scores", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "GRU-latent", "label": "GRU-latent", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "representation learning", "label": "representation learning", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Demon attack", "label": "Demon attack", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "stochastic behaviours", "label": "stochastic behaviours", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "better predictions", "label": "better predictions", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "latent variable", "label": "latent variable", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "future predictions", "label": "future predictions", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "BYOL", "label": "BYOL", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "reward probing", "label": "reward probing", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Pretraining Setup", "label": "Pretraining Setup", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "pretraining setups", "label": "pretraining setups", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "success", "label": "success", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "reward probing task", "label": "reward probing task", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "reward prediction", "label": "reward prediction", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "prediction task", "label": "prediction task", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "pretraining methodology", "label": "pretraining methodology", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "reinforcement learning performance", "label": "reinforcement learning performance", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "results", "label": "results", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "downstream performance", "label": "downstream performance", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Neural predictive belief representations", "label": "Neural predictive belief representations", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Mastering atari with discrete world models", "label": "Mastering atari with discrete world models", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Deep reinforcement learning that matters", "label": "Deep reinforcement learning that matters", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Rainbow: Combining improvements in deep reinforcement learning", "label": "Rainbow: Combining improvements in deep reinforcement learning", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Reward", "label": "Reward", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Reward Prediction F1 Score", "label": "Reward Prediction F1 Score", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Mean RL Performance", "label": "Mean RL Performance", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Accurate Low-Variance Estimate", "label": "Accurate Low-Variance Estimate", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "AtariARI", "label": "AtariARI", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Domain Specific Probing Benchmarks", "label": "Domain Specific Probing Benchmarks", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Video Game Environment", "label": "Video Game Environment", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "MineRL Competition", "label": "MineRL Competition", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Sample E\ufb03cient Reinforcement Learning", "label": "Sample E\ufb03cient Reinforcement Learning", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Human Prior", "label": "Human Prior", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "ObtainDiamond task", "label": "ObtainDiamond task", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "self-driving vehicles", "label": "self-driving vehicles", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Imitation Learning", "label": "Imitation Learning", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "environment sample-complexity", "label": "environment sample-complexity", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Deep Q-Network", "label": "Deep Q-Network", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "sample-efficient methods", "label": "sample-efficient methods", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Minecraft", "label": "Minecraft", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "real-world robotics", "label": "real-world robotics", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Performance Metric", "label": "Performance Metric", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Expert Demonstration", "label": "Expert Demonstration", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "higher reward per episode", "label": "higher reward per episode", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Generalized Advantage Estimation", "label": "Generalized Advantage Estimation", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Competition", "label": "Competition", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "baselines on state of the art RL algorithms", "label": "baselines on state of the art RL algorithms", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Agent", "label": "Agent", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Expert Agent", "label": "Expert Agent", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Ruslan Salakhutdinov", "label": "Ruslan Salakhutdinov", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "University of Toronto", "label": "University of Toronto", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Carnegie Mellon University", "label": "Carnegie Mellon University", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Nicholay Topin", "label": "Nicholay Topin", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Manuela Veloso", "label": "Manuela Veloso", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "JPMorgan Chase", "label": "JPMorgan Chase", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Chelsea Finn", "label": "Chelsea Finn", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Google Brain", "label": "Google Brain", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "UC Berkeley", "label": "UC Berkeley", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Sergey Levine", "label": "Sergey Levine", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Harm van Seijen", "label": "Harm van Seijen", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Microsoft Research", "label": "Microsoft Research", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Oriol Vinyals", "label": "Oriol Vinyals", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Google DeepMind", "label": "Google DeepMind", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "academic researchers", "label": "academic researchers", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "cloud computing resource", "label": "cloud computing resource", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "competition", "label": "competition", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Preferred Networks, Inc.", "label": "Preferred Networks, Inc.", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "company behind Chainer", "label": "company behind Chainer", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "value equivalence (VE)", "label": "value equivalence (VE)", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "proper value equivalence (PVE)", "label": "proper value equivalence (PVE)", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "multiple models", "label": "multiple models", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "agent", "label": "agent", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "optimal policy", "label": "optimal policy", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "set of VE models", "label": "set of VE models", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Bellman operators", "label": "Bellman operators", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "correct result", "label": "correct result", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Proper Value Equivalence Model", "label": "Proper Value Equivalence Model", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Order-k Value Equivalence", "label": "Order-k Value Equivalence", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Value Equivalence Class", "label": "Value Equivalence Class", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Bellman Operator", "label": "Bellman Operator", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "MuZero", "label": "MuZero", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Model", "label": "Model", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "performance", "label": "performance", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Behavior Policy", "label": "Behavior Policy", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "success in the environment", "label": "success in the environment", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Monte-Carlo Tree Search", "label": "Monte-Carlo Tree Search", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "value function estimates", "label": "value function estimates", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Ring Markov Decision Process", "label": "Ring Markov Decision Process", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "False-ring Markov Decision Process", "label": "False-ring Markov Decision Process", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "n-step Bellman Operator", "label": "n-step Bellman Operator", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Episodic Reward", "label": "Episodic Reward", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Deep Neural Networks", "label": "Deep Neural Networks", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Policy Optimization", "label": "Policy Optimization", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Temporal Credit Assignment", "label": "Temporal Credit Assignment", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Reward Function", "label": "Reward Function", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Transformer Model", "label": "Transformer Model", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Proximal Policy Optimization (PPO)", "label": "Proximal Policy Optimization (PPO)", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "state-the-art performance", "label": "state-the-art performance", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "policy gradient", "label": "policy gradient", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "generalized advantage estimation (GAE)", "label": "generalized advantage estimation (GAE)", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "reducing variance", "label": "reducing variance", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "policy gradient methods", "label": "policy gradient methods", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "value function estimation", "label": "value function estimation", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "policy improvement", "label": "policy improvement", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "trajectory", "label": "trajectory", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "final state", "label": "final state", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Proximal Point Algorithm", "label": "Proximal Point Algorithm", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "policy parameters", "label": "policy parameters", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Language Model", "label": "Language Model", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "multi-head attention layer", "label": "multi-head attention layer", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "hidden representation", "label": "hidden representation", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Neural Network Model", "label": "Neural Network Model", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "PPO algorithm", "label": "PPO algorithm", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Environment", "label": "Environment", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Simulator", "label": "Simulator", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Dataset", "label": "Dataset", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "episodic return baselines", "label": "episodic return baselines", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Credit Assignment Algorithm", "label": "Credit Assignment Algorithm", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "sample efficiency", "label": "sample efficiency", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "reinforcement learning techniques", "label": "reinforcement learning techniques", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "training algorithms", "label": "training algorithms", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "insights", "label": "insights", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Hindsight Experience Replay", "label": "Hindsight Experience Replay", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "additional goals", "label": "additional goals", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Reward Shaping Method", "label": "Reward Shaping Method", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "reward function modifications", "label": "reward function modifications", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "memory reconstitutive module", "label": "memory reconstitutive module", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Value Equivalence Principle", "label": "Value Equivalence Principle", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Policies", "label": "Policies", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Model Capacity Constraint", "label": "Model Capacity Constraint", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "value-equivalent models", "label": "value-equivalent models", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "functionally identical", "label": "functionally identical", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "model m", "label": "model m", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Bellman operators T\u03c0", "label": "Bellman operators T\u03c0", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "space M(\u03a0,V)", "label": "space M(\u03a0,V)", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "all models in M", "label": "all models in M", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "set of policies \u03a0", "label": "set of policies \u03a0", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "perfect model", "label": "perfect model", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Control Strategy", "label": "Control Strategy", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "V AML", "label": "V AML", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "ours", "label": "ours", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Joseph et al.", "label": "Joseph et al.", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "algorithm", "label": "algorithm", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Ayoub et al.", "label": "Ayoub et al.", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "value-targeted regression estimation", "label": "value-targeted regression estimation", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "model-based RL", "label": "model-based RL", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "value equivalence principle", "label": "value equivalence principle", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "tailored models", "label": "tailored models", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Exploration Strategy", "label": "Exploration Strategy", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Optimal Adversary", "label": "Optimal Adversary", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "History-Based Feature Abstraction", "label": "History-Based Feature Abstraction", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Continuous Control Problem", "label": "Continuous Control Problem", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Actor Critic Algorithm", "label": "Actor Critic Algorithm", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Memory-based Dynamic Programming Method", "label": "Memory-based Dynamic Programming Method", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "feedforward baseline", "label": "feedforward baseline", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "history-based encoding policies", "label": "history-based encoding policies", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Energy Distance", "label": "Energy Distance", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "better performance", "label": "better performance", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Wasserstein Distance", "label": "Wasserstein Distance", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "MMD", "label": "MMD", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "LSTM-based agent architecture", "label": "LSTM-based agent architecture", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "superior performance", "label": "superior performance", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "AIS", "label": "AIS", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "epistemic state", "label": "epistemic state", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "environment proxy \u03d2", "label": "environment proxy \u03d2", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "entropic measures", "label": "entropic measures", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "system\u2019s uncertainty", "label": "system\u2019s uncertainty", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "bisimulation metrics", "label": "bisimulation metrics", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "operator on the space of semi-metrics", "label": "operator on the space of semi-metrics", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "history-based policies", "label": "history-based policies", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "DeepMDP framework", "label": "DeepMDP framework", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "RL algorithms", "label": "RL algorithms", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "attention mechanism", "label": "attention mechanism", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Generative Adversarial Network", "label": "Generative Adversarial Network", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Reinforcement Learning", "label": "Reinforcement Learning", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "SQIL Algorithm", "label": "SQIL Algorithm", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Soft Q Imitation Learning", "label": "Soft Q Imitation Learning", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "SQIL", "label": "SQIL", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "BC", "label": "BC", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "competitive results", "label": "competitive results", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "state distribution shift problem", "label": "state distribution shift problem", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Soft Value Function", "label": "Soft Value Function", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Behavioral Cloning Method", "label": "Behavioral Cloning Method", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Soft Q values", "label": "Soft Q values", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "GAIL", "label": "GAIL", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "discriminator", "label": "discriminator", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "constant reward", "label": "constant reward", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "SAC", "label": "SAC", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "continuous actions", "label": "continuous actions", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "RBC", "label": "RBC", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "imitation policy", "label": "imitation policy", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "demonstrations", "label": "demonstrations", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Lunar Lander game", "label": "Lunar Lander game", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "evaluation", "label": "evaluation", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Absorbing State", "label": "Absorbing State", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "GAIL-DQL-B", "label": "GAIL-DQL-B", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "GAIL-TRPO-B", "label": "GAIL-TRPO-B", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "GAIL-DQL-U", "label": "GAIL-DQL-U", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "GAIL-TRPO-U", "label": "GAIL-TRPO-U", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Demonstration Rollouts", "label": "Demonstration Rollouts", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Policy Optimization from Demonstration Method", "label": "Policy Optimization from Demonstration Method", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Sparse Reward Environment", "label": "Sparse Reward Environment", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Demonstrations", "label": "Demonstrations", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Demonstration Data", "label": "Demonstration Data", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Generative Adversarial Imitation Learning Algorithm", "label": "Generative Adversarial Imitation Learning Algorithm", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Expert Policy", "label": "Expert Policy", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Generative Adversarial Imitation Learning", "label": "Generative Adversarial Imitation Learning", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "MDP", "label": "MDP", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "state space", "label": "state space", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "State Action Pair", "label": "State Action Pair", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Policy", "label": "Policy", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Policy Optimization with Demonstrations", "label": "Policy Optimization with Demonstrations", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Demonstrated Trajectories", "label": "Demonstrated Trajectories", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Learning Objective", "label": "Learning Objective", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Occupancy Measure", "label": "Occupancy Measure", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Optimization Process", "label": "Optimization Process", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Adversarial Training Method", "label": "Adversarial Training Method", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Jensen-Shannon Divergence", "label": "Jensen-Shannon Divergence", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Discriminator Model", "label": "Discriminator Model", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Replay Memory", "label": "Replay Memory", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Experience Replay Buffer", "label": "Experience Replay Buffer", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "POfD", "label": "POfD", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "expert-level performance", "label": "expert-level performance", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "TRPO", "label": "TRPO", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "sparse environments", "label": "sparse environments", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "DQfD", "label": "DQfD", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "imperfect demonstration data", "label": "imperfect demonstration data", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Humanoid-v1", "label": "Humanoid-v1", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Reacher-v1", "label": "Reacher-v1", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Learning from Demonstration Method", "label": "Learning from Demonstration Method", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Craftable Item", "label": "Craftable Item", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "MineRL", "label": "MineRL", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "human demonstrations", "label": "human demonstrations", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "dataset", "label": "dataset", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Wood", "label": "Wood", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Tools", "label": "Tools", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Iron Pickaxe", "label": "Iron Pickaxe", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Key materials", "label": "Key materials", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Diamonds", "label": "Diamonds", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "High-level Minecraft play", "label": "High-level Minecraft play", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Cooked meat", "label": "Cooked meat", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "replenish stamina", "label": "replenish stamina", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Bed", "label": "Bed", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Sleeping", "label": "Sleeping", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Hierarchical Reinforcement Learning Architecture", "label": "Hierarchical Reinforcement Learning Architecture", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Obtain\u003cItem\u003e", "label": "Obtain\u003cItem\u003e", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "ObtainIronPickaxe", "label": "ObtainIronPickaxe", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "ObtainDiamond", "label": "ObtainDiamond", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "iron ore", "label": "iron ore", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "ObtainCookedMeat", "label": "ObtainCookedMeat", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "State Association Learning Method", "label": "State Association Learning Method", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Synthetic Return", "label": "Synthetic Return", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Atari Skiing", "label": "Atari Skiing", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "State Representation", "label": "State Representation", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "State", "label": "State", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "key-pickup", "label": "key-pickup", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "SA-learning", "label": "SA-learning", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "long-term credit assignment", "label": "long-term credit assignment", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "opened door", "label": "opened door", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "receive reward", "label": "receive reward", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "buffer of state representations", "label": "buffer of state representations", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "predict future rewards", "label": "predict future rewards", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Trigger State", "label": "Trigger State", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Example-Based Control", "label": "Example-Based Control", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Recursive Classification", "label": "Recursive Classification", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Example-Based Policy Search", "label": "Example-Based Policy Search", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Success Example", "label": "Success Example", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Robust Example-Based Control", "label": "Robust Example-Based Control", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Probability", "label": "Probability", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "example-based control", "label": "example-based control", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Algorithm", "label": "Algorithm", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "two-stage approaches", "label": "two-stage approaches", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "ValueDICE", "label": "ValueDICE", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "KL Loss Function", "label": "KL Loss Function", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "C-learning", "label": "C-learning", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "example_string", "label": "example_string", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "imitation learning method", "label": "imitation learning method", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Transition Distribution", "label": "Transition Distribution", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Future Success Classifier", "label": "Future Success Classifier", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Bayes-optimal Classifier", "label": "Bayes-optimal Classifier", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Tabular Model", "label": "Tabular Model", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Value Iteration Algorithm", "label": "Value Iteration Algorithm", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Probability of Success", "label": "Probability of Success", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "data-driven approach", "label": "data-driven approach", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "reinforcement learning", "label": "reinforcement learning", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Temporal Difference Method", "label": "Temporal Difference Method", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "expected value of rewards", "label": "expected value of rewards", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "iterated RCE", "label": "iterated RCE", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "success examples", "label": "success examples", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "objective function", "label": "objective function", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "learning a reward function", "label": "learning a reward function", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "density model", "label": "density model", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "auxiliary function approximators", "label": "auxiliary function approximators", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "n-step returns", "label": "n-step returns", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "current policy", "label": "current policy", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "historical average policy", "label": "historical average policy", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "performance on sawyer_push and sawyer_drawer_open tasks", "label": "performance on sawyer_push and sawyer_drawer_open tasks", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "RCE", "label": "RCE", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "behavior policy", "label": "behavior policy", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "implicit reward", "label": "implicit reward", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Agent-Temporal Attention for Reward Redistribution in Episodic Multi-Agent Reinforcement Learning", "label": "Agent-Temporal Attention for Reward Redistribution in Episodic Multi-Agent Reinforcement Learning", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "multi-agent credit assignment problem", "label": "multi-agent credit assignment problem", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "AREL", "label": "AREL", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "higher rewards", "label": "higher rewards", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "win rates", "label": "win rates", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "any given MARL algorithm", "label": "any given MARL algorithm", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "multi-agent reinforcement learning (MARL)", "label": "multi-agent reinforcement learning (MARL)", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "multiple autonomous agents", "label": "multiple autonomous agents", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "reward signal", "label": "reward signal", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Temporal Attention Module", "label": "Temporal Attention Module", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Attention Mechanism", "label": "Attention Mechanism", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Agent-Temporal Attention Block", "label": "Agent-Temporal Attention Block", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Multi-Agent Reinforcement Learning", "label": "Multi-Agent Reinforcement Learning", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "multi-agent credit assignment", "label": "multi-agent credit assignment", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "delayed rewards", "label": "delayed rewards", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "scalability challenges", "label": "scalability challenges", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Value decomposition networks", "label": "Value decomposition networks", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "centralized value", "label": "centralized value", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "QTRAN", "label": "QTRAN", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "credit assignment", "label": "credit assignment", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Centralized Training with Decentralized Execution Paradigm", "label": "Centralized Training with Decentralized Execution Paradigm", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Decentralized Policies", "label": "Decentralized Policies", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Effective Multi-Agent Temporal Credit Assignment", "label": "Effective Multi-Agent Temporal Credit Assignment", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Agent Attention Module", "label": "Agent Attention Module", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Agent-Group Embedding", "label": "Agent-Group Embedding", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Identification of Agents", "label": "Identification of Agents", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Sparse Rewards", "label": "Sparse Rewards", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Temporal Decomposition of Reward", "label": "Temporal Decomposition of Reward", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Contributions of Agents", "label": "Contributions of Agents", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Multi-Agent System", "label": "Multi-Agent System", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Episode", "label": "Episode", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "State Space", "label": "State Space", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Decentralized Partially Observable Sequence-Markov Decision Process", "label": "Decentralized Partially Observable Sequence-Markov Decision Process", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Optimal Policy", "label": "Optimal Policy", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Sequence Modeling", "label": "Sequence Modeling", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "episodic reward", "label": "episodic reward", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "MARL algorithm", "label": "MARL algorithm", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "input", "label": "input", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "effective credit assignment", "label": "effective credit assignment", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": " redistributed rewards", "label": " redistributed rewards", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "multi-head attention", "label": "multi-head attention", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Particle World", "label": "Particle World", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "StarCraft", "label": "StarCraft", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "average rewards", "label": "average rewards", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "QMIX", "label": "QMIX", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "base algorithm", "label": "base algorithm", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "deep recurrent Q-network", "label": "deep recurrent Q-network", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "RMSprop", "label": "RMSprop", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "energy consumption", "label": "energy consumption", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "experience buffer", "label": "experience buffer", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "trajectories", "label": "trajectories", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "training episodes", "label": "training episodes", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "goal-directed behavior", "label": "goal-directed behavior", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Intrinsic Motivation", "label": "Intrinsic Motivation", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "exploration strategy", "label": "exploration strategy", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Hierarchical Deep Reinforcement Learning", "label": "Hierarchical Deep Reinforcement Learning", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "goal-driven intrinsically motivated deep reinforcement learning", "label": "goal-driven intrinsically motivated deep reinforcement learning", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "h-DQN", "label": "h-DQN", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "effective exploration in sparse environments", "label": "effective exploration in sparse environments", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Hierarchical action-value functions", "label": "Hierarchical action-value functions", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "sparse feedback challenge", "label": "sparse feedback challenge", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Meta Controller", "label": "Meta Controller", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "action-value function", "label": "action-value function", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Controller", "label": "Controller", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "policy over actions", "label": "policy over actions", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "states", "label": "states", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "raw states", "label": "raw states", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "Internal Critic", "label": "Internal Critic", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "positive reward", "label": "positive reward", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "cumulative intrinsic reward", "label": "cumulative intrinsic reward", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "cumulative extrinsic reward", "label": "cumulative extrinsic reward", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "external reward", "label": "external reward", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "goal", "label": "goal", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "reward", "label": "reward", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "experience replay memoriesD1", "label": "experience replay memoriesD1", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "106", "label": "106", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "experience replay memoriesD2", "label": "experience replay memoriesD2", "shape": "dot", "title": "CONTENT"}, {"color": "#F1C40F", "font": {"color": "black"}, "id": "5 \u00b7104", "label": "5 \u00b7104", "shape": "dot", "title": "CONTENT"}]);
                  edges = new vis.DataSet([{"from": "Reinforcement Learning Algorithm", "label": "transforms", "to": "Return Decomposition Technique", "width": 2.55}, {"from": "Markov Decision Process", "label": "improves", "to": "Optimal Reward Redistribution", "width": 2.7}, {"from": "Reward Redistribution Method", "label": "yields", "to": "Sequence-Markov Decision Process", "width": 2.4000000000000004}, {"from": "Return Decomposition Technique", "label": "integrates", "to": "Contribution Analysis", "width": 2.25}, {"from": "Delayed Reward Mechanism", "label": "addresses", "to": "Delayed Reward Mechanism", "width": 2.0999999999999996}, {"from": "Reward Redistribution Method", "label": "enables", "to": "Return Decomposition Technique", "width": 2.55}, {"from": "Return Decomposition Technique", "label": "yields", "to": "Q-learning Method", "width": 2.4000000000000004}, {"from": "Advantage Function", "label": "optimizes", "to": "Reward Redistribution Method", "width": 2.25}, {"from": "Temporal Value Transport Mechanism", "label": "integrates", "to": "Return Decomposition Technique", "width": 2.34}, {"from": "Markov Decision Process", "label": "described by", "to": "Reward Redistribution Method", "width": 2.4000000000000004}, {"from": "Sequence-Markov Decision Process", "label": "is equivalent to", "to": "Markov Decision Process", "width": 2.25}, {"from": "Optimal Reward Redistribution", "label": "enables", "to": "expected future rewards", "width": 2.7}, {"from": "Delayed Reward Mechanism", "label": "minimizes", "to": "expected future rewards", "width": 2.4000000000000004}, {"from": "Bellman Equation", "label": "yields", "to": "Q-value estimation", "width": 2.7}, {"from": "Reinforcement Learning Algorithm", "label": "is equivalent to", "to": "Long Short-Term Memory Network", "width": 2.55}, {"from": "RUDDER", "label": "optimizes", "to": "Reinforcement Learning Algorithm", "width": 2.4000000000000004}, {"from": "Atari games", "label": "is fixed point of", "to": "Value Function", "width": 2.25}, {"from": "RUDDER", "label": "enables", "to": "Reward Redistribution Method", "width": 2.7}, {"from": "Human Expert Episodes", "label": "contributes to", "to": "Reinforcement Learning Algorithm", "width": 2.46}, {"from": "RUDDER", "label": "outperforms", "to": "Q(\u03bb)", "width": 2.8499999999999996}, {"from": "RUDDER", "label": "outperforms", "to": "MC", "width": 2.8499999999999996}, {"from": "RUDDER", "label": "outperforms", "to": "MCTS", "width": 2.8499999999999996}, {"from": "RUDDER", "label": "supports", "to": "Supplements Theorem S8", "width": 2.55}, {"from": "Reinforcement Learning Algorithm", "label": "improves", "to": "PPO baseline", "width": 2.7}, {"from": "RUDDER", "label": "outperforms", "to": "TD(\u03bb)", "width": 2.8499999999999996}, {"from": "RUDDER", "label": "outperforms", "to": "reward shaping methods", "width": 2.8499999999999996}, {"from": "RUDDER", "label": "increases", "to": "scores", "width": 2.55}, {"from": "RUDDER", "label": "controls", "to": "learning time", "width": 2.7}, {"from": "RUDDER", "label": "utilizes", "to": "reward redistribution", "width": 2.4000000000000004}, {"from": "Delayed Reward Mechanism", "label": "improves", "to": "scores", "width": 2.7}, {"from": "Feudal Network", "label": "transforms", "to": "Hierarchical Reinforcement Learning", "width": 2.55}, {"from": "Manager Module", "label": "operates_at", "to": "lower temporal resolution", "width": 2.7}, {"from": "Worker Module", "label": "generates", "to": "primitive actions", "width": 2.64}, {"from": "Feudal Network", "label": "enables", "to": "temporal sub-policies", "width": 2.4000000000000004}, {"from": "Manager Module", "label": "sets", "to": "abstract goals", "width": 2.7600000000000002}, {"from": "Worker Module", "label": "enacts", "to": "goals", "width": 2.61}, {"from": "Goal", "label": "is fixed point of", "to": "Dilated LSTM", "width": 2.55}, {"from": "Reinforcement Learning Algorithm", "label": "addresses", "to": "Transition Policy", "width": 2.4000000000000004}, {"from": "Action Primitive", "label": "enables", "to": "Goal", "width": 2.25}, {"from": "Recurrent Neural Network", "label": "integrates", "to": "Dilated LSTM", "width": 2.7}, {"from": "Task", "label": "correlates", "to": "Goal", "width": 2.64}, {"from": "FuN", "label": "is equivalent to", "to": "Feudal Network", "width": 2.55}, {"from": "Manager Module", "label": "controls", "to": "Goal Embedding", "width": 2.25}, {"from": "Worker Module", "label": "produces", "to": "actions", "width": 2.4000000000000004}, {"from": "Goal", "label": "modulates", "to": "policy", "width": 2.0999999999999996}, {"from": "Manager Module", "label": "outputs", "to": "goal vector", "width": 2.34}, {"from": "Worker Module", "label": "works with", "to": "Manager Module", "width": 2.31}, {"from": "FuN", "label": "outperforms", "to": "LSTM", "width": 2.55}, {"from": "T-maze+", "label": "is prerequisite for", "to": "Non-match", "width": 2.25}, {"from": "Ablative analysis", "label": "validates", "to": "transition policy gradient for training the Manager", "width": 2.4000000000000004}, {"from": "LSTM", "label": "fails to utilize", "to": "memory for water maze task", "width": 2.0999999999999996}, {"from": "Feudal Network", "label": "is equivalent to", "to": "Reinforcement Learning Algorithm", "width": 2.55}, {"from": "Transition Policy", "label": "guides", "to": "Worker Module", "width": 2.25}, {"from": "Gaussian Distribution", "label": "describes", "to": "Policy Gradient Method", "width": 2.34}, {"from": "Intrinsic Reward", "label": "enhances", "to": "Feudal Network", "width": 2.46}, {"from": "FuN", "label": "transforms", "to": "Manager Module", "width": 2.4000000000000004}, {"from": "Manager Module", "label": "enables", "to": "Transition Policy", "width": 2.55}, {"from": "Worker Module", "label": "integrates", "to": "Primitive Actions", "width": 2.25}, {"from": "Transition Policy", "label": "is prerequisite for", "to": "Behavioural Primitive", "width": 2.4000000000000004}, {"from": "Reinforcement Learning Algorithm", "label": "enables", "to": "Unsupervised visual representation learning", "width": 2.55}, {"from": "Reward Redistribution Method", "label": "addresses", "to": "the challenge of computationally intensive training", "width": 2.4000000000000004}, {"from": "Reinforcement Learning Algorithm", "label": "yields", "to": "better methods for exploring pretraining algorithms", "width": 2.25}, {"from": "Evaluation Protocol", "label": "improves", "to": "existing self-supervised learning recipes", "width": 2.7}, {"from": "Linear Probing Task", "label": "correlates", "to": "actual downstream control performance", "width": 2.64}, {"from": "Reinforcement Learning Algorithm", "label": "correlates", "to": "Reward Probing Method", "width": 2.55}, {"from": "Reward Probing Method", "label": "is fixed point of", "to": "Linear Probing Task", "width": 2.4000000000000004}, {"from": "Unsupervised Learning Method", "label": "enables", "to": "Reinforcement Learning Algorithm", "width": 2.25}, {"from": "Reward Predicting Method", "label": "correlates", "to": "Value Prediction", "width": 2.55}, {"from": "Reward Prediction", "label": "is prerequisite for", "to": "Value-Based Control", "width": 2.25}, {"from": "Reward Prediction", "label": "supports", "to": "Critic in Actor-Critic Methods", "width": 2.4000000000000004}, {"from": "Data Collection Methodology", "label": "is essential for", "to": "Action Prediction", "width": 2.7}, {"from": "Transition Model", "label": "optimizes", "to": "Representation Learning", "width": 2.34}, {"from": "Recurrent Neural Network", "label": "improves", "to": "probing performance", "width": 2.55}, {"from": "Barlow", "label": "is prerequisite for", "to": "probing scores", "width": 2.34}, {"from": "GRU-latent", "label": "transforms", "to": "representation learning", "width": 2.4000000000000004}, {"from": "Demon attack", "label": "exhibits", "to": "stochastic behaviours", "width": 2.2800000000000002}, {"from": "GRU-latent", "label": "yields", "to": "better predictions", "width": 2.46}, {"from": "latent variable", "label": "guides", "to": "future predictions", "width": 2.37}, {"from": "BYOL", "label": "augments", "to": "reward probing", "width": 2.31}, {"from": "Reward Probing Method", "label": "is equivalent to", "to": "Pretraining Setup", "width": 2.4000000000000004}, {"from": "Reward Probing Method", "label": "guides", "to": "pretraining setups", "width": 2.55}, {"from": "success", "label": "is prerequisite for", "to": "reward probing task", "width": 2.25}, {"from": "reward prediction", "label": "influences", "to": "prediction task", "width": 2.0999999999999996}, {"from": "pretraining methodology", "label": "improves", "to": "reinforcement learning performance", "width": 2.7}, {"from": "results", "label": "addresses", "to": "downstream performance", "width": 2.4000000000000004}, {"from": "Neural predictive belief representations", "label": "explains", "to": "Reinforcement Learning Algorithm", "width": 2.55}, {"from": "Mastering atari with discrete world models", "label": "is equivalent to", "to": "Reinforcement Learning Algorithm", "width": 2.25}, {"from": "Deep reinforcement learning that matters", "label": "promotes", "to": "Reinforcement Learning Algorithm", "width": 2.4000000000000004}, {"from": "Rainbow: Combining improvements in deep reinforcement learning", "label": "transforms", "to": "Reinforcement Learning Algorithm", "width": 2.7}, {"from": "Reward", "label": "predicts", "to": "Reinforcement Learning Algorithm", "width": 2.55}, {"from": "Reward Prediction F1 Score", "label": "correlates", "to": "Mean RL Performance", "width": 2.4000000000000004}, {"from": "Reward Prediction F1 Score", "label": "yields", "to": "Accurate Low-Variance Estimate", "width": 2.25}, {"from": "AtariARI", "label": "compares", "to": "Domain Specific Probing Benchmarks", "width": 2.0999999999999996}, {"from": "Reinforcement Learning Algorithm", "label": "enables", "to": "Video Game Environment", "width": 2.4000000000000004}, {"from": "MineRL Competition", "label": "promotes", "to": "Sample E\ufb03cient Reinforcement Learning", "width": 2.25}, {"from": "Human Prior", "label": "guides", "to": "Reinforcement Learning Algorithm", "width": 2.55}, {"from": "ObtainDiamond task", "label": "requires", "to": "Reinforcement Learning Algorithm", "width": 2.0999999999999996}, {"from": "Reinforcement Learning Algorithm", "label": "applies", "to": "self-driving vehicles", "width": 2.55}, {"from": "Imitation Learning", "label": "reduces", "to": "environment sample-complexity", "width": 2.7}, {"from": "Deep Q-Network", "label": "is prerequisite for", "to": "sample-efficient methods", "width": 2.4000000000000004}, {"from": "Minecraft", "label": "is equivalent to", "to": "real-world robotics", "width": 2.7}, {"from": "Reinforcement Learning Algorithm", "label": "improves", "to": "Performance Metric", "width": 2.55}, {"from": "Expert Demonstration", "label": "yields", "to": "higher reward per episode", "width": 2.7}, {"from": "Human Prior", "label": "is prerequisite for", "to": "Generalized Advantage Estimation", "width": 2.4000000000000004}, {"from": "Competition", "label": "provides", "to": "baselines on state of the art RL algorithms", "width": 2.25}, {"from": "Agent", "label": "is equivalent to", "to": "Expert Agent", "width": 2.0999999999999996}, {"from": "Ruslan Salakhutdinov", "label": "is affiliated with", "to": "University of Toronto", "width": 2.7}, {"from": "Ruslan Salakhutdinov", "label": "is affiliated with", "to": "Carnegie Mellon University", "width": 2.7}, {"from": "Nicholay Topin", "label": "is advised by", "to": "Manuela Veloso", "width": 2.55}, {"from": "Manuela Veloso", "label": "is affiliated with", "to": "Carnegie Mellon University", "width": 2.8499999999999996}, {"from": "Manuela Veloso", "label": "is affiliated with", "to": "JPMorgan Chase", "width": 2.55}, {"from": "Chelsea Finn", "label": "is affiliated with", "to": "Google Brain", "width": 2.7}, {"from": "Chelsea Finn", "label": "is affiliated with", "to": "UC Berkeley", "width": 2.7}, {"from": "Sergey Levine", "label": "is affiliated with", "to": "UC Berkeley", "width": 2.7}, {"from": "Harm van Seijen", "label": "affiliated with", "to": "Microsoft Research", "width": 2.55}, {"from": "Oriol Vinyals", "label": "affiliated with", "to": "Google DeepMind", "width": 2.55}, {"from": "Microsoft Research", "label": "collaborates with", "to": "academic researchers", "width": 2.25}, {"from": "Microsoft Research", "label": "provides", "to": "cloud computing resource", "width": 2.7}, {"from": "Microsoft Research", "label": "supports", "to": "competition", "width": 2.7}, {"from": "Preferred Networks, Inc.", "label": "is equivalent to", "to": "company behind Chainer", "width": 2.4000000000000004}, {"from": "value equivalence (VE)", "label": "defines", "to": "proper value equivalence (PVE)", "width": 2.55}, {"from": "proper value equivalence (PVE)", "label": "contains", "to": "multiple models", "width": 2.4000000000000004}, {"from": "agent", "label": "yields", "to": "optimal policy", "width": 2.7}, {"from": "value equivalence (VE)", "label": "shrinks", "to": "set of VE models", "width": 2.25}, {"from": "Bellman operators", "label": "induce", "to": "correct result", "width": 2.64}, {"from": "Proper Value Equivalence Model", "label": "is equivalent to", "to": "Value Function", "width": 2.55}, {"from": "Order-k Value Equivalence", "label": "is prerequisite for", "to": "Value Equivalence Class", "width": 2.4000000000000004}, {"from": "Markov Decision Process", "label": "is fixed point of", "to": "Bellman Operator", "width": 2.25}, {"from": "Bellman Equation", "label": "guides", "to": "Reinforcement Learning Algorithm", "width": 2.7}, {"from": "Optimal Reward Redistribution", "label": "transforms", "to": "Reward Redistribution Method", "width": 2.55}, {"from": "Reinforcement Learning Algorithm", "label": "is equivalent to", "to": "MuZero", "width": 2.7}, {"from": "Model", "label": "optimizes", "to": "performance", "width": 2.55}, {"from": "Behavior Policy", "label": "is prerequisite for", "to": "success in the environment", "width": 2.4000000000000004}, {"from": "Monte-Carlo Tree Search", "label": "integrates", "to": "value function estimates", "width": 2.25}, {"from": "Ring Markov Decision Process", "label": "relies on", "to": "Reward Redistribution Method", "width": 2.55}, {"from": "False-ring Markov Decision Process", "label": "mimics", "to": "Ring Markov Decision Process", "width": 2.4000000000000004}, {"from": "n-step Bellman Operator", "label": "is prerequisite for", "to": "Optimal Reward Redistribution", "width": 2.7}, {"from": "Reinforcement Learning Algorithm", "label": "transforms", "to": "Reward Redistribution Method", "width": 2.55}, {"from": "Episodic Reward", "label": "decomposes", "to": "Return Decomposition Technique", "width": 2.25}, {"from": "Deep Neural Networks", "label": "guides", "to": "Policy Optimization", "width": 2.7}, {"from": "Temporal Credit Assignment", "label": "addresses", "to": "Delayed Reward Mechanism", "width": 2.4000000000000004}, {"from": "Reinforcement Learning Algorithm", "label": "transforms", "to": "Reward Function", "width": 2.55}, {"from": "Sequence-Markov Decision Process", "label": "supports", "to": "Transformer Model", "width": 2.25}, {"from": "Reward Redistribution Method", "label": "decomposes", "to": "Reward Function", "width": 2.34}, {"from": "Proximal Policy Optimization (PPO)", "label": "yields", "to": "state-the-art performance", "width": 2.55}, {"from": "Proximal Policy Optimization (PPO)", "label": "regularizes", "to": "policy gradient", "width": 2.25}, {"from": "generalized advantage estimation (GAE)", "label": "helps", "to": "reducing variance", "width": 2.4000000000000004}, {"from": "policy gradient methods", "label": "is prerequisite for", "to": "value function estimation", "width": 2.0999999999999996}, {"from": "policy gradient methods", "label": "improves", "to": "policy improvement", "width": 2.16}, {"from": "trajectory", "label": "captures", "to": "final state", "width": 2.25}, {"from": "Proximal Point Algorithm", "label": "optimizes", "to": "policy parameters", "width": 2.55}, {"from": "Transformer Model", "label": "is fixed point of", "to": "Language Model", "width": 2.55}, {"from": "Transformer Model", "label": "integrates", "to": "Recurrent Neural Network", "width": 2.25}, {"from": "Transformer Model", "label": "uses", "to": "multi-head attention layer", "width": 2.7}, {"from": "multi-head attention layer", "label": "outputs", "to": "hidden representation", "width": 2.4000000000000004}, {"from": "Reinforcement Learning Algorithm", "label": "influences", "to": "Neural Network Model", "width": 2.55}, {"from": "PPO algorithm", "label": "yields", "to": "Reward Function", "width": 2.7}, {"from": "Generalized Advantage Estimation", "label": "optimizes", "to": "Policy Gradient Method", "width": 2.64}, {"from": "Environment", "label": "is prerequisite for", "to": "Reward Redistribution Method", "width": 2.25}, {"from": "Simulator", "label": "provides", "to": "Dataset", "width": 2.4000000000000004}, {"from": "Return Decomposition Technique", "label": "outperforms", "to": "episodic return baselines", "width": 2.7}, {"from": "Reinforcement Learning Algorithm", "label": "transforms", "to": "Credit Assignment Algorithm", "width": 2.25}, {"from": "Transformer Model", "label": "outperforms", "to": "LSTM", "width": 2.64}, {"from": "Credit Assignment Algorithm", "label": "improves", "to": "sample efficiency", "width": 2.4000000000000004}, {"from": "Reward Function", "label": "integrates", "to": "reinforcement learning techniques", "width": 2.0999999999999996}, {"from": "Reinforcement Learning Algorithm", "label": "improves", "to": "sample efficiency", "width": 2.55}, {"from": "Intrinsic Reward", "label": "guides", "to": "training algorithms", "width": 2.25}, {"from": "Credit Assignment Algorithm", "label": "provides", "to": "insights", "width": 2.4000000000000004}, {"from": "Hindsight Experience Replay", "label": "adds", "to": "additional goals", "width": 2.0999999999999996}, {"from": "Reward Shaping Method", "label": "explores", "to": "reward function modifications", "width": 2.16}, {"from": "Temporal Value Transport Mechanism", "label": "relies on", "to": "memory reconstitutive module", "width": 2.13}, {"from": "Value Equivalence Principle", "label": "is equivalent to", "to": "Proper Value Equivalence Model", "width": 2.7}, {"from": "Value Equivalence Principle", "label": "addresses", "to": "Reinforcement Learning Algorithm", "width": 2.4000000000000004}, {"from": "Bellman Operator", "label": "yields", "to": "Value Function", "width": 2.55}, {"from": "Bellman Operator", "label": "modulates", "to": "Q-learning Method", "width": 2.25}, {"from": "Value Equivalence Principle", "label": "transforms", "to": "Optimal Reward Redistribution", "width": 2.4000000000000004}, {"from": "Policies", "label": "influences", "to": "Model Capacity Constraint", "width": 2.0999999999999996}, {"from": "value-equivalent models", "label": "is equivalent to", "to": "functionally identical", "width": 2.7}, {"from": "model m", "label": "is fixed point of", "to": "Bellman operators T\u03c0", "width": 2.55}, {"from": "space M(\u03a0,V)", "label": "contains", "to": "all models in M", "width": 2.64}, {"from": "set of policies \u03a0", "label": "corresponds to", "to": "perfect model", "width": 2.25}, {"from": "Reinforcement Learning Algorithm", "label": "utilizes", "to": "Markov Decision Process", "width": 2.55}, {"from": "Markov Decision Process", "label": "influences", "to": "Value Function", "width": 2.4000000000000004}, {"from": "Value Function", "label": "is prerequisite for", "to": "Bellman Equation", "width": 2.25}, {"from": "Bellman Equation", "label": "provides", "to": "Optimal Reward Redistribution", "width": 2.34}, {"from": "Value Equivalence Principle", "label": "guides", "to": "Control Strategy", "width": 2.25}, {"from": "V AML", "label": "complements", "to": "ours", "width": 2.55}, {"from": "Joseph et al.", "label": "proposes", "to": "algorithm", "width": 2.7}, {"from": "Ayoub et al.", "label": "proposes", "to": "algorithm", "width": 2.7600000000000002}, {"from": "value-targeted regression estimation", "label": "is sufficient for", "to": "model-based RL", "width": 2.4000000000000004}, {"from": "value equivalence principle", "label": "is equivalent to", "to": "tailored models", "width": 2.64}, {"from": "Value Function", "label": "is equivalent to", "to": "Value Equivalence Class", "width": 2.55}, {"from": "Markov Decision Process", "label": "optimizes", "to": "Reward Function", "width": 2.4000000000000004}, {"from": "Reinforcement Learning Algorithm", "label": "integrates", "to": "Exploration Strategy", "width": 2.25}, {"from": "Optimal Adversary", "label": "is fixed point of", "to": "Bellman Operator", "width": 2.0999999999999996}, {"from": "History-Based Feature Abstraction", "label": "improves", "to": "Reinforcement Learning Algorithm", "width": 2.7}, {"from": "Reinforcement Learning Algorithm", "label": "controls", "to": "Continuous Control Problem", "width": 2.55}, {"from": "Actor Critic Algorithm", "label": "is equivalent to", "to": "Deep Q-Network", "width": 2.64}, {"from": "Policy Gradient Method", "label": "outperforms", "to": "Memory-based Dynamic Programming Method", "width": 2.61}, {"from": "Reinforcement Learning Algorithm", "label": "improves", "to": "performance", "width": 2.4000000000000004}, {"from": "Reinforcement Learning Algorithm", "label": "outperforms", "to": "feedforward baseline", "width": 2.25}, {"from": "Reinforcement Learning Algorithm", "label": "lends credence to", "to": "history-based encoding policies", "width": 2.0999999999999996}, {"from": "Energy Distance", "label": "yields", "to": "better performance", "width": 2.55}, {"from": "Wasserstein Distance", "label": "compared to", "to": "MMD", "width": 2.7}, {"from": "LSTM-based agent architecture", "label": "results in", "to": "superior performance", "width": 2.34}, {"from": "AIS", "label": "is equivalent to", "to": "epistemic state", "width": 2.55}, {"from": "AIS", "label": "represents", "to": "environment proxy \u03d2", "width": 2.4000000000000004}, {"from": "entropic measures", "label": "captures", "to": "system\u2019s uncertainty", "width": 2.25}, {"from": "bisimulation metrics", "label": "is fixed point of", "to": "operator on the space of semi-metrics", "width": 2.7}, {"from": "history-based policies", "label": "extends", "to": "DeepMDP framework", "width": 2.64}, {"from": "RL algorithms", "label": "uses", "to": "attention mechanism", "width": 2.7600000000000002}, {"from": "Policy Gradient Method", "label": "optimizes", "to": "Reinforcement Learning Algorithm", "width": 2.25}, {"from": "Deep Q-Network", "label": "yields", "to": "Markov Decision Process", "width": 2.55}, {"from": "Generative Adversarial Network", "label": "is fixed point of", "to": "Reinforcement Learning Algorithm", "width": 2.34}, {"from": "Reinforcement Learning", "label": "transforms", "to": "Imitation Learning", "width": 2.55}, {"from": "SQIL Algorithm", "label": "is equivalent to", "to": "Imitation Learning", "width": 2.7}, {"from": "Imitation Learning", "label": "addresses", "to": "Expert Agent", "width": 2.34}, {"from": "Expert Demonstration", "label": "enables", "to": "Reinforcement Learning", "width": 2.4000000000000004}, {"from": "Soft Q Imitation Learning", "label": "is equivalent to", "to": "SQIL", "width": 2.7}, {"from": "SQIL", "label": "outperforms", "to": "BC", "width": 2.55}, {"from": "SQIL", "label": "achieves", "to": "competitive results", "width": 2.4000000000000004}, {"from": "SQIL", "label": "can overcome", "to": "state distribution shift problem", "width": 2.25}, {"from": "Soft Value Function", "label": "is equivalent to", "to": "Reward Function", "width": 2.25}, {"from": "Behavioral Cloning Method", "label": "improves", "to": "Imitation Learning", "width": 2.4000000000000004}, {"from": "Soft Q values", "label": "correlates", "to": "Reward Function", "width": 2.0999999999999996}, {"from": "Expert Demonstration", "label": "provides", "to": "Imitation Learning", "width": 2.7}, {"from": "SQIL Algorithm", "label": "outperforms", "to": "BC", "width": 2.55}, {"from": "SQIL Algorithm", "label": "is equivalent to", "to": "GAIL", "width": 2.25}, {"from": "GAIL", "label": "outperforms", "to": "BC", "width": 2.0999999999999996}, {"from": "GAIL", "label": "struggles", "to": "discriminator", "width": 2.4000000000000004}, {"from": "SQIL", "label": "benefits from", "to": "constant reward", "width": 2.25}, {"from": "Expert Demonstration", "label": "is prerequisite for", "to": "SAC", "width": 2.0999999999999996}, {"from": "SQIL", "label": "can be adapted to", "to": "continuous actions", "width": 2.34}, {"from": "SQIL Algorithm", "label": "outperforms", "to": "RBC", "width": 2.55}, {"from": "SQIL Algorithm", "label": "relies on", "to": "imitation policy", "width": 2.7}, {"from": "SQIL Algorithm", "label": "transforms", "to": "demonstrations", "width": 2.25}, {"from": "SQIL Algorithm", "label": "generalizes", "to": "performance", "width": 2.4000000000000004}, {"from": "Lunar Lander game", "label": "is fixed point of", "to": "evaluation", "width": 2.31}, {"from": "SQIL Algorithm", "label": "outperforms", "to": "Behavioral Cloning Method", "width": 2.55}, {"from": "SQIL Algorithm", "label": "achieves", "to": "competitive results", "width": 2.4000000000000004}, {"from": "SQIL Algorithm", "label": "is equivalent to", "to": "Generalized Advantage Estimation", "width": 2.25}, {"from": "Absorbing State", "label": "is prerequisite for", "to": "Soft Value Function", "width": 2.55}, {"from": "GAIL-DQL-B", "label": "is equivalent to", "to": "GAIL-TRPO-B", "width": 2.34}, {"from": "GAIL-DQL-U", "label": "is equivalent to", "to": "GAIL-TRPO-U", "width": 2.34}, {"from": "SQIL Algorithm", "label": "correlates", "to": "Reward Function", "width": 2.4000000000000004}, {"from": "Demonstration Rollouts", "label": "yields", "to": "Expert Demonstration", "width": 2.64}, {"from": "Policy Optimization from Demonstration Method", "label": "influences", "to": "Reinforcement Learning Algorithm", "width": 2.55}, {"from": "Policy Optimization from Demonstration Method", "label": "guides", "to": "Exploration Strategy", "width": 2.25}, {"from": "Policy Optimization from Demonstration Method", "label": "optimizes", "to": "Sparse Reward Environment", "width": 2.34}, {"from": "Demonstrations", "label": "guides", "to": "Reinforcement Learning Algorithm", "width": 2.4000000000000004}, {"from": "Demonstration Data", "label": "is prerequisite for", "to": "Policy Optimization from Demonstration Method", "width": 2.0999999999999996}, {"from": "Policy Optimization from Demonstration Method", "label": "transforms", "to": "Policy Gradient Method", "width": 2.55}, {"from": "Generative Adversarial Imitation Learning Algorithm", "label": "works_with", "to": "Expert Policy", "width": 2.4000000000000004}, {"from": "Imitation Learning", "label": "enables", "to": "Reward Shaping Method", "width": 2.0999999999999996}, {"from": "Reward Redistribution Method", "label": "improves", "to": "Sparse Reward Environment", "width": 2.34}, {"from": "Generative Adversarial Imitation Learning", "label": "optimizes", "to": "policy", "width": 2.55}, {"from": "GAIL", "label": "is equivalent to", "to": "Imitation Learning", "width": 2.25}, {"from": "MDP", "label": "defines", "to": "state space", "width": 2.7}, {"from": "State Action Pair", "label": "is fixed point of", "to": "Policy", "width": 2.0999999999999996}, {"from": "Reinforcement Learning Algorithm", "label": "enables", "to": "Policy Optimization with Demonstrations", "width": 2.55}, {"from": "Expert Policy", "label": "guides", "to": "Agent", "width": 2.4000000000000004}, {"from": "Sparse Reward Environment", "label": "poses challenges for", "to": "Reinforcement Learning Algorithm", "width": 2.0999999999999996}, {"from": "Expert Demonstration", "label": "leverages", "to": "Policy Optimization from Demonstration Method", "width": 2.25}, {"from": "Demonstrated Trajectories", "label": "inspire", "to": "Agent", "width": 2.4000000000000004}, {"from": "Policy", "label": "defines", "to": "Learning Objective", "width": 2.55}, {"from": "Occupancy Measure", "label": "guides", "to": "Optimization Process", "width": 2.4000000000000004}, {"from": "Adversarial Training Method", "label": "optimizes", "to": "Policy", "width": 2.7}, {"from": "Policy", "label": "integrates", "to": "Expert Demonstration", "width": 2.25}, {"from": "Jensen-Shannon Divergence", "label": "compares", "to": "Policy", "width": 2.0999999999999996}, {"from": "Generative Adversarial Network", "label": "is prerequisite for", "to": "Policy Gradient Method", "width": 2.55}, {"from": "Discriminator Model", "label": "optimizes", "to": "Generative Adversarial Network", "width": 2.34}, {"from": "Expert Policy", "label": "guides", "to": "Generative Adversarial Network", "width": 2.7}, {"from": "Expert Demonstration", "label": "aids", "to": "Reinforcement Learning Algorithm", "width": 2.4000000000000004}, {"from": "Deep Q-Network", "label": "is prerequisite for", "to": "Q-learning Method", "width": 2.55}, {"from": "Deep Q-Network", "label": "optimizes", "to": "Reward Function", "width": 2.7}, {"from": "Replay Memory", "label": "is fixed point of", "to": "Experience Replay Buffer", "width": 2.4000000000000004}, {"from": "POfD", "label": "outperforms", "to": "GAIL", "width": 2.55}, {"from": "POfD", "label": "achieves", "to": "expert-level performance", "width": 2.7}, {"from": "TRPO", "label": "fails to explore", "to": "sparse environments", "width": 2.8499999999999996}, {"from": "DQfD", "label": "converges to", "to": "imperfect demonstration data", "width": 2.64}, {"from": "Humanoid-v1", "label": "is prerequisite for", "to": "Reinforcement Learning Algorithm", "width": 2.4000000000000004}, {"from": "Reacher-v1", "label": "is equivalent to", "to": "Sparse Reward Environment", "width": 2.25}, {"from": "POfD", "label": "improves", "to": "Learning from Demonstration Method", "width": 2.7}, {"from": "Dataset", "label": "enables", "to": "Reinforcement Learning Algorithm", "width": 2.7}, {"from": "Minecraft", "label": "is fixed point of", "to": "Video Game Environment", "width": 2.4000000000000004}, {"from": "Action Primitive", "label": "is equivalent to", "to": "Craftable Item", "width": 2.25}, {"from": "MineRL", "label": "promotes", "to": "Imitation Learning", "width": 2.64}, {"from": "Minecraft", "label": "involves", "to": "Exploration Strategy", "width": 2.7}, {"from": "Minecraft", "label": "includes", "to": "Action Primitive", "width": 2.55}, {"from": "human demonstrations", "label": "recorded during", "to": "Minecraft", "width": 2.4000000000000004}, {"from": "MineRL", "label": "provides", "to": "dataset", "width": 2.7600000000000002}, {"from": "Wood", "label": "is prerequisite for", "to": "Tools", "width": 2.55}, {"from": "Iron Pickaxe", "label": "obtains", "to": "Key materials", "width": 2.25}, {"from": "Diamonds", "label": "is equivalent to", "to": "High-level Minecraft play", "width": 2.4000000000000004}, {"from": "Cooked meat", "label": "is used to", "to": "replenish stamina", "width": 2.34}, {"from": "Bed", "label": "is required for", "to": "Sleeping", "width": 2.46}, {"from": "Reinforcement Learning Algorithm", "label": "enables", "to": "Hierarchical Reinforcement Learning Architecture", "width": 2.4000000000000004}, {"from": "Obtain\u003cItem\u003e", "label": "is prerequisite for", "to": "ObtainIronPickaxe", "width": 2.25}, {"from": "ObtainDiamond", "label": "overlaps with", "to": "ObtainIronPickaxe", "width": 2.0999999999999996}, {"from": "ObtainIronPickaxe", "label": "requires", "to": "iron ore", "width": 2.55}, {"from": "ObtainCookedMeat", "label": "does not require", "to": "iron ore", "width": 2.4000000000000004}, {"from": "Agent", "label": "uses", "to": "State Association Learning Method", "width": 2.7}, {"from": "State Association Learning Method", "label": "yields", "to": "Synthetic Return", "width": 2.55}, {"from": "Synthetic Return", "label": "optimizes", "to": "Policy", "width": 2.4000000000000004}, {"from": "Agent", "label": "augments", "to": "Reinforcement Learning Algorithm", "width": 2.4000000000000004}, {"from": "Reinforcement Learning Algorithm", "label": "solves", "to": "Atari Skiing", "width": 2.25}, {"from": "Agent", "label": "learns", "to": "State Representation", "width": 2.7}, {"from": "State Representation", "label": "predicts", "to": "Reward", "width": 2.7}, {"from": "State Association Learning Method", "label": "assigns_credit_to", "to": "State", "width": 2.4000000000000004}, {"from": "Agent", "label": "assigns_credit_to", "to": "key-pickup", "width": 2.55}, {"from": "SA-learning", "label": "optimizes", "to": "long-term credit assignment", "width": 2.7}, {"from": "opened door", "label": "is prerequisite for", "to": "receive reward", "width": 2.4000000000000004}, {"from": "buffer of state representations", "label": "aids", "to": "predict future rewards", "width": 2.25}, {"from": "Reinforcement Learning Algorithm", "label": "influences", "to": "Synthetic Return", "width": 2.55}, {"from": "Agent", "label": "yields", "to": "Reward", "width": 2.7}, {"from": "Agent", "label": "addresses", "to": "Delayed Reward Mechanism", "width": 2.4000000000000004}, {"from": "Agent", "label": "correlates", "to": "Trigger State", "width": 2.25}, {"from": "Reinforcement Learning Algorithm", "label": "guides", "to": "State Association Learning Method", "width": 2.0999999999999996}, {"from": "Sequence-Markov Decision Process", "label": "complements", "to": "Reinforcement Learning Algorithm", "width": 2.25}, {"from": "Example-Based Control", "label": "promotes", "to": "Reinforcement Learning Algorithm", "width": 2.55}, {"from": "Example-Based Control", "label": "guides", "to": "Reward Function", "width": 2.4000000000000004}, {"from": "Control Strategy", "label": "integrates", "to": "Recursive Classification", "width": 2.0999999999999996}, {"from": "Example-Based Policy Search", "label": "yields", "to": "Success Example", "width": 2.7}, {"from": "Reward Function", "label": "replaces", "to": "Success Example", "width": 2.55}, {"from": "Robust Example-Based Control", "label": "yields", "to": "Bellman Equation", "width": 2.25}, {"from": "Success Example", "label": "converges_to", "to": "Value Function", "width": 2.4000000000000004}, {"from": "Policy", "label": "maximizes", "to": "Probability", "width": 2.34}, {"from": "example-based control", "label": "studies", "to": "Reward Function", "width": 2.0999999999999996}, {"from": "Algorithm", "label": "outperforms", "to": "two-stage approaches", "width": 2.64}, {"from": "Example-Based Control", "label": "is equivalent to", "to": "Goal", "width": 2.55}, {"from": "ValueDICE", "label": "is fixed point of", "to": "KL Loss Function", "width": 2.7}, {"from": "C-learning", "label": "influences", "to": "Example-Based Control", "width": 2.4000000000000004}, {"from": "example_string", "label": "generalizes", "to": "imitation learning method", "width": 2.25}, {"from": "Success Example", "label": "is fixed point of", "to": "Policy", "width": 2.0999999999999996}, {"from": "Transition Distribution", "label": "provides", "to": "Data Collection Methodology", "width": 2.25}, {"from": "Future Success Classifier", "label": "predicts", "to": "Success Example", "width": 2.55}, {"from": "Future Success Classifier", "label": "enables", "to": "Example-Based Control", "width": 2.7}, {"from": "Recursive Classification", "label": "is prerequisite for", "to": "Future Success Classifier", "width": 2.25}, {"from": "Bayes-optimal Classifier", "label": "integrates", "to": "Future Success Classifier", "width": 2.4000000000000004}, {"from": "Reinforcement Learning Algorithm", "label": "enables", "to": "Robust Example-Based Control", "width": 2.55}, {"from": "Success Example", "label": "correlates", "to": "Bayes-optimal Classifier", "width": 2.25}, {"from": "Reward Function", "label": "optimizes", "to": "Policy", "width": 2.34}, {"from": "Tabular Model", "label": "converges_to", "to": "Value Iteration Algorithm", "width": 2.4000000000000004}, {"from": "Policy", "label": "improves", "to": "Performance Metric", "width": 2.7}, {"from": "Imitation Learning", "label": "optimizes", "to": "policy", "width": 2.25}, {"from": "policy", "label": "maximizes", "to": "Probability of Success", "width": 2.7}, {"from": "data-driven approach", "label": "complements", "to": "reinforcement learning", "width": 2.34}, {"from": "Value Iteration Algorithm", "label": "is equivalent to", "to": "Temporal Difference Method", "width": 2.4000000000000004}, {"from": "Bellman Equation", "label": "provides", "to": "expected value of rewards", "width": 2.8499999999999996}, {"from": "policy", "label": "is fixed point of", "to": "iterated RCE", "width": 2.55}, {"from": "success examples", "label": "correlates", "to": "objective function", "width": 2.25}, {"from": "learning a reward function", "label": "transforms", "to": "density model", "width": 2.0999999999999996}, {"from": "example-based control", "label": "depends on", "to": "auxiliary function approximators", "width": 2.4000000000000004}, {"from": "n-step returns", "label": "improves", "to": "performance", "width": 2.55}, {"from": "current policy", "label": "outperforms", "to": "historical average policy", "width": 2.4000000000000004}, {"from": "n-step returns", "label": "improves", "to": "performance on sawyer_push and sawyer_drawer_open tasks", "width": 2.7}, {"from": "RCE", "label": "works at least as well as", "to": "behavior policy", "width": 2.25}, {"from": "RCE", "label": "regularizes", "to": "implicit reward", "width": 2.0999999999999996}, {"from": "Agent-Temporal Attention for Reward Redistribution in Episodic Multi-Agent Reinforcement Learning", "label": "addresses", "to": "multi-agent credit assignment problem", "width": 2.7}, {"from": "AREL", "label": "yields", "to": "higher rewards", "width": 2.55}, {"from": "AREL", "label": "improves", "to": "win rates", "width": 2.55}, {"from": "AREL", "label": "integrates", "to": "any given MARL algorithm", "width": 2.4000000000000004}, {"from": "multi-agent reinforcement learning (MARL)", "label": "involves", "to": "multiple autonomous agents", "width": 2.8499999999999996}, {"from": "AREL", "label": "transforms", "to": "reward signal", "width": 2.25}, {"from": "Reinforcement Learning Algorithm", "label": "addresses", "to": "Temporal Attention Module", "width": 2.55}, {"from": "Agent", "label": "collaborates with", "to": "Agent", "width": 2.25}, {"from": "Attention Mechanism", "label": "enables", "to": "Agent-Temporal Attention Block", "width": 2.7}, {"from": "Temporal Attention Module", "label": "modulates", "to": "Episodic Reward", "width": 2.46}, {"from": "Multi-Agent Reinforcement Learning", "label": "is prerequisite for", "to": "Reward Redistribution Method", "width": 2.4000000000000004}, {"from": "Attention Mechanism", "label": "enables", "to": "multi-agent credit assignment", "width": 2.55}, {"from": "multi-agent credit assignment", "label": "addresses", "to": "delayed rewards", "width": 2.4000000000000004}, {"from": "AREL", "label": "overcomes", "to": "scalability challenges", "width": 2.7}, {"from": "Value decomposition networks", "label": "decomposes", "to": "centralized value", "width": 2.25}, {"from": "QTRAN", "label": "improves", "to": "credit assignment", "width": 2.0999999999999996}, {"from": "Centralized Training with Decentralized Execution Paradigm", "label": "is prerequisite for", "to": "Reward Redistribution Method", "width": 2.55}, {"from": "Reward Redistribution Method", "label": "addresses", "to": "Temporal Attention Module", "width": 2.4000000000000004}, {"from": "Agent-Temporal Attention Block", "label": "influences", "to": "Decentralized Policies", "width": 2.25}, {"from": "Decentralized Policies", "label": "yields", "to": "Effective Multi-Agent Temporal Credit Assignment", "width": 2.7}, {"from": "Temporal Attention Module", "label": "modulates", "to": "Agent Attention Module", "width": 2.34}, {"from": "Agent", "label": "shares", "to": "Agent-Group Embedding", "width": 2.55}, {"from": "Reinforcement Learning Algorithm", "label": "enables", "to": "Identification of Agents", "width": 2.4000000000000004}, {"from": "Reward Redistribution Method", "label": "addresses", "to": "Sparse Rewards", "width": 2.25}, {"from": "Temporal Decomposition of Reward", "label": "assesses", "to": "Contributions of Agents", "width": 2.34}, {"from": "Reinforcement Learning Algorithm", "label": "transforms", "to": "Multi-Agent System", "width": 2.46}, {"from": "Episode", "label": "correlates", "to": "State Space", "width": 2.4000000000000004}, {"from": "Decentralized Partially Observable Sequence-Markov Decision Process", "label": "is equivalent to", "to": "Decentralized Partially Observable Sequence-Markov Decision Process", "width": 3.0}, {"from": "Decentralized Partially Observable Sequence-Markov Decision Process", "label": "predicts", "to": "Optimal Policy", "width": 2.4000000000000004}, {"from": "Reinforcement Learning Algorithm", "label": "guides", "to": "Task", "width": 2.34}, {"from": "Attention Mechanism", "label": "improves", "to": "Reward Function", "width": 2.7}, {"from": "RUDDER", "label": "outperforms", "to": "Sequence Modeling", "width": 2.25}, {"from": "Agent", "label": "interacts with", "to": "Environment", "width": 2.64}, {"from": "Reward Redistribution Method", "label": "yields", "to": "episodic reward", "width": 2.34}, {"from": "MARL algorithm", "label": "learns", "to": "Value Function", "width": 2.4000000000000004}, {"from": "AREL", "label": "provides", "to": "input", "width": 2.46}, {"from": "Attention Mechanism", "label": "is prerequisite for", "to": "effective credit assignment", "width": 2.25}, {"from": "Reinforcement Learning Algorithm", "label": "enables", "to": "Attention Mechanism", "width": 2.4000000000000004}, {"from": "Credit Assignment Algorithm", "label": "yields", "to": " redistributed rewards", "width": 2.25}, {"from": "Transformer Model", "label": "applies", "to": "multi-head attention", "width": 2.55}, {"from": "Agent", "label": "experiments with", "to": "Particle World", "width": 2.0999999999999996}, {"from": "Agent", "label": "competes in", "to": "StarCraft", "width": 2.0999999999999996}, {"from": "Attention Mechanism", "label": "improves", "to": "average rewards", "width": 2.55}, {"from": "QMIX", "label": "is equivalent to", "to": "base algorithm", "width": 2.4000000000000004}, {"from": "QMIX", "label": "utilizes", "to": "deep recurrent Q-network", "width": 2.7}, {"from": "QMIX", "label": "trains", "to": "RMSprop", "width": 2.25}, {"from": "AREL", "label": "addresses", "to": "energy consumption", "width": 2.0999999999999996}, {"from": "experience buffer", "label": "fills", "to": "trajectories", "width": 2.34}, {"from": "trajectory", "label": "is part of", "to": "training episodes", "width": 2.46}, {"from": "Hierarchical Reinforcement Learning Architecture", "label": "enables", "to": "goal-directed behavior", "width": 2.55}, {"from": "Intrinsic Motivation", "label": "improves", "to": "exploration strategy", "width": 2.25}, {"from": "Hierarchical Deep Reinforcement Learning", "label": "integrates", "to": "goal-driven intrinsically motivated deep reinforcement learning", "width": 2.7}, {"from": "h-DQN", "label": "yields", "to": "effective exploration in sparse environments", "width": 2.4000000000000004}, {"from": "Hierarchical action-value functions", "label": "addresses", "to": "sparse feedback challenge", "width": 2.0999999999999996}, {"from": "Meta Controller", "label": "estimates", "to": "action-value function", "width": 2.55}, {"from": "Controller", "label": "produces", "to": "policy over actions", "width": 2.64}, {"from": "Controller", "label": "takes in", "to": "states", "width": 2.7}, {"from": "Meta Controller", "label": "looks at", "to": "raw states", "width": 2.58}, {"from": "Internal Critic", "label": "provides", "to": "positive reward", "width": 2.61}, {"from": "Reward Function", "label": "maximizes", "to": "cumulative intrinsic reward", "width": 2.52}, {"from": "Meta Controller", "label": "optimizes", "to": "cumulative extrinsic reward", "width": 2.67}, {"from": "Reward Redistribution Method", "label": "addresses", "to": "external reward", "width": 2.43}, {"from": "Reinforcement Learning Algorithm", "label": "integrates", "to": "Q-learning Method", "width": 2.55}, {"from": "State", "label": "is prerequisite for", "to": "Optimal Reward Redistribution", "width": 2.25}, {"from": "Agent", "label": "yields", "to": "Intrinsic Reward", "width": 2.7}, {"from": "Meta Controller", "label": "controls", "to": "Controller", "width": 2.8499999999999996}, {"from": "Controller", "label": "satisfies", "to": "goal", "width": 2.7}, {"from": "goal", "label": "is prerequisite for", "to": "reward", "width": 2.55}, {"from": "experience replay memoriesD1", "label": "is fixed point of", "to": "106", "width": 2.4000000000000004}, {"from": "experience replay memoriesD2", "label": "is fixed point of", "to": "5 \u00b7104", "width": 2.4000000000000004}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "forceAtlas2Based": {
            "avoidOverlap": 0,
            "centralGravity": 0.01,
            "damping": 0.4,
            "gravitationalConstant": -50,
            "springConstant": 0.08,
            "springLength": 100
        },
        "solver": "forceAtlas2Based",
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  
                      network.on("stabilizationProgress", function(params) {
                          document.getElementById('loadingBar').removeAttribute("style");
                          var maxWidth = 496;
                          var minWidth = 20;
                          var widthFactor = params.iterations/params.total;
                          var width = Math.max(minWidth,maxWidth * widthFactor);
                          document.getElementById('bar').style.width = width + 'px';
                          document.getElementById('text').innerHTML = Math.round(widthFactor*100) + '%';
                      });
                      network.once("stabilizationIterationsDone", function() {
                          document.getElementById('text').innerHTML = '100%';
                          document.getElementById('bar').style.width = '496px';
                          document.getElementById('loadingBar').style.opacity = 0;
                          // really clean the dom element
                          setTimeout(function () {document.getElementById('loadingBar').style.display = 'none';}, 500);
                      });
                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>