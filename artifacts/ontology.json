{
  "competency_questions": [
    "How can delayed rewards be effectively redistributed to simplify Q-value estimation?",
    "How does return decomposition contribute to more efficient credit assignment in reinforcement learning?",
    "How can optimal reward redistribution be implemented to achieve expected future rewards equal to zero?",
    "How can a delayed reward MDP be transformed into a return\u2010equivalent SDP using a second order Markov reward redistribution?",
    "How can non\u2010optimal reward redistributions be utilized to ensure convergence to optimal policies in reinforcement learning?",
    "How can the explaining away problem be mitigated through the definition of a difference function between state-action pairs?",
    "How can recurrent neural networks be utilized to achieve more accurate return predictions for effective reward redistribution?",
    "What approaches can mitigate spurious reward signals caused by prediction errors in feedforward neural networks?",
    "How can safe exploration strategies be integrated into reinforcement learning to effectively manage delayed rewards?",
    "How can hierarchical reinforcement learning architectures enhance long-term credit assignment?",
    "How does decoupling goal generation from action execution impact learning efficiency in hierarchical reinforcement learning?",
    "How does the modular interaction between Manager and Worker impact learning dynamics in hierarchical reinforcement learning?",
    "How can intrinsic rewards be effectively utilized to encourage directional shifts in latent state representations in hierarchical reinforcement learning?",
    "How can transition policy gradients be utilized to optimize Manager updates in hierarchical reinforcement learning?",
    "What are the benefits of employing a dilated LSTM architecture for long-term memory preservation in hierarchical reinforcement learning?",
    "How can training recurrent neural networks over extended sequences enhance memory-related performance in reinforcement learning?",
    "How do hierarchical reinforcement learning architectures that incorporate sub-policy mechanisms compare to flat architectures in terms of learning efficiency and memory integration?",
    "How can transition policies be effectively transferred between agents with different action repeat configurations?",
    "How does temporal resolution affect the learning dynamics of Manager and Worker modules in hierarchical reinforcement learning?",
    "How does the decoupling of sub-goal discovery from primitive action generation affect learning stability and efficiency in hierarchical reinforcement learning?",
    "How can learned behavioural primitives be effectively transferred to agents with different embodiment to facilitate multi-task learning?",
    "How can linear probing tasks be used to efficiently evaluate unsupervised visual representations for reinforcement learning?",
    "How can the correlation between linear probing and reinforcement learning performance be explicitly demonstrated?",
    "How can sequence modeling architectures, such as GRU, improve the modeling of state transitions in partially observable reinforcement learning environments?",
    "How does the choice of transition model affect the quality of learned state representations in reinforcement learning?",
    "What is the impact of different auxiliary self-supervised learning objectives on both probing outcomes and downstream RL performance?",
    "How do encoder and decoder architectures influence the efficiency of pre-training and representation learning?",
    "What are the limitations and requirements of implementing reward probing methods in reinforcement learning systems?",
    "How can reward probing techniques be extended to diverse environments to enhance reinforcement learning pre-training?",
    "How does the introduction of latent variables in recurrent models improve prediction probing performance in reinforcement learning?",
    "How do auxiliary self-supervised learning objectives influence forward prediction and representation quality in reinforcement learning?",
    "How do domain-specific probing benchmarks compare with domain-agnostic reward probing in estimating RL performance?",
    "How does the variability in bootstrap-derived confidence intervals relate to the number of independent experimental runs?",
    "How can feudal control mechanisms be designed to effectively delegate sub-tasks and propagate rewards across multiple hierarchical levels?",
    "How do reward hiding strategies influence the learning dynamics of sub-managers in hierarchical reinforcement learning systems?",
    "How do information hiding practices affect the flow of state information and task delegation in hierarchical reinforcement learning architectures?",
    "How can hierarchical reinforcement learning architectures adapt to non-Markovian dynamics in large state spaces?",
    "How can human demonstration data be effectively leveraged to reduce the number of required samples in reinforcement learning tasks?",
    "How can competition frameworks be designed to foster reproducible, democratized, and efficient development of reinforcement learning methods?",
    "How can reinforcement learning techniques be adapted to handle dynamic, embodied environments like Minecraft?",
    "How can methods developed in the Minecraft domain be transferred to address challenges in real-world robotics?",
    "How do prerequisite relationships between game items affect task progression in dynamic game environments?",
    "How can strict sample complexity constraints be managed to design computationally efficient sequential decision making algorithms in reinforcement learning?",
    "How can the integration of automated and manual evaluation processes ensure compliance with competition rules?",
    "How can sandboxed execution environments be utilized to prevent information leaks during evaluation?",
    "How can AI research be made accessible to a larger community of engineers?",
    "How can prize distribution mechanisms incentivize community engagement in competitions?",
    "How can facility resource allocation be optimized to effectively support academic workshops?",
    "How can travel grants and scholarships be structured to increase participation from underrepresented groups?",
    "How can the smallest sets of policies and value functions be selected to ensure that a model is sufficient for planning in reinforcement learning?",
    "How does proper value equivalence allow for the existence of multiple planning-sufficient models in reinforcement learning without collapsing to a singleton?",
    "How do irrelevant environmental features affect the expansion of value equivalence classes in reinforcement learning?",
    "How does increasing the VE order influence the diversity of planning-sufficient models in reinforcement learning?",
    "How do value equivalent model classes defined over all policies compare with those defined over deterministic policies in reinforcement learning?",
    "What are the computational trade-offs between enforcing proper value equivalence and order-k value equivalence in reinforcement learning?",
    "How can MuZero\u2019s loss function minimization be interpreted as minimizing a squared planning value equivalence (PVE) loss in model-based reinforcement learning?",
    "How does increasing the model order (k) in value equivalence affect the characteristics of the planning-sufficient model space?",
    "How do agent capacity constraints influence the planning performance differences between M\u221e(\u03a0) and M\u221e(\u03a0det)?",
    "How do discrepancies in transition dynamics between PVE models and the environment impact model fidelity?",
    "How does the reliance on Bellman fixed-points in PVE models eliminate the need for specifying a set of functions to define a VE class?",
    "How does ensuring PVE with respect to all deterministic policies guarantee optimal planning in the environment?",
    "How does the k-step Bellman operator differentiate reward accumulation between ring and false-ring MDPs?",
    "How can differences between Mk(\u03a0,V) and M\u221e(\u03a0) inform the design of planning-sufficient models in reinforcement learning?",
    "How can transformer-based sequence models be leveraged to improve the decomposition and interpretability of episodic rewards in reinforcement learning?",
    "How can a dense surrogate reward function be designed to effectively approximate temporal credit assignment in episodic reinforcement learning?",
    "How can neural-network language models be utilized to learn reward functions for improved temporal credit assignment in reinforcement learning?",
    "How can residual bias correction techniques enhance unbiased policy gradient estimation in reinforcement learning?",
    "Is the learned reward function useful for improving policy optimization in episodic RL?",
    "What are the appropriate choices of interval set, neural network model, and dataset for accurate reward prediction in episodic reinforcement learning?",
    "How do varying learning rate settings impact the effectiveness of bias correction techniques in credit assignment for reinforcement learning?",
    "How does the choice of network structure (Transformer, LSTM, Feedforward) impact learning performance in reinforcement learning?",
    "How can temporal attention patterns be leveraged to improve interpretability of learned reward functions in reinforcement learning?",
    "How can value equivalence loss be minimized using sample transitions in model-based reinforcement learning?",
    "How do multiplicative interactions between pointwise linearly independent policies and linearly independent value functions bound the dimension of value-equivalent model spaces?",
    "How can the geometric properties of the value polytope be leveraged to define a spanning set of value functions in reinforcement learning?",
    "How can nonlinear function approximation be leveraged to enhance the performance of value equivalence-based reinforcement learning models?",
    "How can value equivalence be leveraged to induce compatible state representations for improved representation learning in reinforcement learning systems?",
    "How can experimental pipelines be designed to validate theoretical guarantees and demonstrate practical performance in model-based reinforcement learning?",
    "How does model capacity restriction through rank constraints impact transition dynamics estimation in reinforcement learning models?",
    "How can history-based feature abstraction be theoretically modeled to match the performance of optimal policies in Markov decision processes?",
    "How can history-based reinforcement learning algorithms be analyzed to bridge the gap between theory and practice in continuous control tasks?",
    "How can integral probability metrics be utilized to derive approximation bounds for history-based feature abstractions in reinforcement learning?",
    "How does the incorporation of finite state machine memory influence state visitation and policy performance in reinforcement learning?",
    "How can an AIS-approximator be integrated with standard reinforcement learning algorithms to enhance policy search performance?",
    "How does replacing fully connected layers with a GRU architecture affect reinforcement learning performance and optimization?",
    "How does the choice of energy distance versus Wasserstein distance metrics influence performance evaluation in continuous control tasks?",
    "How can the additional structure of MDP models be leveraged to derive tighter bounds on approximation error compared to POMDPs?",
    "How do design choices in history compression functions influence the approximation error in history-based reinforcement learning methods?",
    "How do Lipschitz constants of reward, transition, and approximator functions influence convergence properties in dynamic programming for reinforcement learning?",
    "How can constant reward signals be employed to facilitate long-horizon imitation learning without resorting to adversarial reward learning?",
    "How does regularization in behavioral cloning mitigate state distribution shift in imitation learning?",
    "How can penalty terms on reward functions incorporate state transition dynamics in imitation learning?",
    "How does the use of constant reward signals versus learned rewards influence the learning efficiency of imitation learning algorithms across different environments?",
    "How can sparse environmental rewards and imperfect expert demonstrations be effectively integrated to optimize policy performance?",
    "How does demonstration-guided exploration leveraging occupancy measure divergence impact advantage and policy improvement in reinforcement learning?",
    "How can adversarial training be applied to optimize divergence-based regularization in policy gradient methods?",
    "How can occupancy measure matching regularization mitigate the underexploitation of limited demonstration data in sparse reward environments?",
    "How can actor\u2010critic frameworks be optimized to improve policy performance when demonstrations and self\u2010generated data are combined in reinforcement learning?",
    "How can demonstration-based intrinsic reward reshaping be integrated with policy gradient methods to enhance exploration in sparse reward environments?",
    "How can continuous data collection mechanisms enhance the scalability and adaptability of reinforcement learning methods in dynamic environments?",
    "How can a predefined set of action primitives be effectively utilized to optimize reinforcement learning performance?",
    "How can learned reward-predictive associations be utilized to generate synthetic returns for long-term credit assignment?",
    "How does incorporating a buffer of state representations affect credit assignment efficiency in deep reinforcement learning?",
    "How does SA-learning complement TD learning for efficient long-term credit assignment in reinforcement learning tasks?",
    "How can distracting events and unrelated rewards during delayed phases be managed to improve credit assignment in reinforcement learning tasks?",
    "How can robustness be enhanced in reinforcement learning methods to mitigate issues arising from substantial seed variance and hyperparameter sensitivity?",
    "How can the integration of complementary reinforcement learning methods lead to significant improvements in state-of-the-art performance and sample efficiency?",
    "How can double counting of rewards be mitigated when combining state-associative learning with TD learning?",
    "How can task specification in reinforcement learning be made data-driven using examples?",
    "How can policies be learned from success examples in off-policy scenarios without relying on conventional reward functions?",
    "How can future success probability estimation via a recursive classification approach improve policy optimization in reinforcement learning?",
    "How does the integration of bootstrapping techniques in positive-unlabeled classification contribute to robust future success predictions?",
    "How does recursive classification of examples contribute to convergence and optimality guarantees in reinforcement learning?",
    "How does minimizing the squared Hellinger distance enhance robustness in example\u2010based control?",
    "How can online data collection be effectively integrated with robust example\u2010based control to improve performance?",
    "How can direct learning of a value function from success examples accelerate policy optimization compared to indirect classifier-based approaches?",
    "How can control problems be reformulated in a data-driven manner to better capture real-world dynamics without relying on traditional reward functions?",
    "How can the fixed point property of an iterative robust example\u2010based control method be ensured in reinforcement learning?",
    "How can attention mechanisms be leveraged to effectively redistribute delayed rewards in multi-agent reinforcement learning?",
    "How can attention modules be designed to capture long-term dependencies for improved temporal credit assignment in episodic multi-agent reinforcement learning?",
    "How can decentralized agent policies be learned in multi-agent systems with delayed episodic rewards without human intervention?",
    "How can episodic rewards be redistributed to achieve effective temporal credit assignment in multi-agent reinforcement learning?",
    "How can agent-temporal attention mechanisms be leveraged to enhance the interpretability and performance of decentralized policies in multi-agent reinforcement learning?",
    "How does the composition of temporal and agent attention modules enhance expressivity in credit assignment across multi-agent systems?",
    "How does permutation invariance in credit assignment modules affect multi-agent reinforcement learning performance?",
    "How can uniform reward redistribution be adapted when the same state appears in different episodes causing conflicting reward assignments?",
    "How does return equivalence in decentralized partially observable sequence-Markov decision processes influence the derivation of optimal policies?",
    "How can the relative contributions of agents at intermediate time-steps be effectively characterized to enhance reward redistribution?",
    "How does the bias-variance trade-off influence the optimization of loss functions in reinforcement learning?",
    "How can estimators be designed to minimize mean square error by balancing bias and variance?",
    "How does Gaussian initialization of parameters influence the variance and concentration of predictions in reinforcement learning?",
    "How can reinforcement learning architectures be optimized to reduce energy consumption when scaling to large numbers of agents?",
    "How can credit assignment mechanisms be effectively updated under varying training schedules in multi-agent environments?",
    "How does the depth of agent-temporal attention blocks influence reward variance and early-stage performance in cooperative multi-agent reinforcement learning?",
    "How do different regularization loss functions impact the sparsity of redistributed rewards and overall learning efficiency in reinforcement learning frameworks?",
    "How do default and delayed reward mechanisms compare in reinforcement learning environments in terms of test win rates and long-term credit assignment?",
    "How can hierarchical deep reinforcement learning architectures leverage intrinsic motivation to overcome sparse feedback challenges?",
    "How can unsupervised object detection be integrated into reinforcement learning for effective goal parameterization?",
    "How can relational intrinsic reward mechanisms be generalized across diverse environments?",
    "How does distinguishing between meta-controller and controller architectures affect overall agent performance?"
  ],
  "entities": [
    {
      "name": "Reinforcement Learning Algorithm"
    },
    {
      "name": "Markov Decision Process"
    },
    {
      "name": "Sequence-Markov Decision Process"
    },
    {
      "name": "Reward Redistribution Method"
    },
    {
      "name": "Return Decomposition Technique"
    },
    {
      "name": "Contribution Analysis"
    },
    {
      "name": "Optimal Reward Redistribution"
    },
    {
      "name": "Bellman Equation"
    },
    {
      "name": "Advantage Function"
    },
    {
      "name": "Eligibility Trace Mechanism"
    },
    {
      "name": "Q-learning Method"
    },
    {
      "name": "Policy Gradient Method"
    },
    {
      "name": "Recurrent Neural Network"
    },
    {
      "name": "Feedforward Neural Network"
    },
    {
      "name": "Video Game Environment"
    },
    {
      "name": "Exploration Strategy"
    },
    {
      "name": "Control Strategy"
    },
    {
      "name": "Error Propagation Network"
    },
    {
      "name": "Imitation Learning"
    },
    {
      "name": "Prioritized Experience Replay"
    },
    {
      "name": "Upside-Down Reinforcement Learning Method"
    },
    {
      "name": "Generalized Advantage Estimation"
    },
    {
      "name": "Axiomatic Attribution Method"
    },
    {
      "name": "Reward Shaping Method"
    },
    {
      "name": "Hierarchical Reinforcement Learning Architecture"
    },
    {
      "name": "Manager Module"
    },
    {
      "name": "Worker Module"
    },
    {
      "name": "Latent State Representation"
    },
    {
      "name": "Goal Embedding"
    },
    {
      "name": "Intrinsic Reward"
    },
    {
      "name": "Transition Policy"
    },
    {
      "name": "Transition Distribution"
    },
    {
      "name": "Dilated LSTM"
    },
    {
      "name": "von Mises-Fisher Distribution"
    },
    {
      "name": "Convolutional Neural Network"
    },
    {
      "name": "Backpropagation Through Time"
    },
    {
      "name": "Sub-policy"
    },
    {
      "name": "Discount Factor"
    },
    {
      "name": "Option-Critic Architecture"
    },
    {
      "name": "Maze Environment"
    },
    {
      "name": "Feudal Network"
    },
    {
      "name": "Action Repeat Mechanism"
    },
    {
      "name": "Continuous Control Problem"
    },
    {
      "name": "Behavioural Primitive"
    },
    {
      "name": "Sub-goal"
    },
    {
      "name": "Macro Action"
    },
    {
      "name": "Option"
    },
    {
      "name": "Dilated Convolution"
    },
    {
      "name": "Attention Mechanism"
    },
    {
      "name": "Temporal Attention Module"
    },
    {
      "name": "Agent Attention Module"
    },
    {
      "name": "Self-supervised Learning Method"
    },
    {
      "name": "Evaluation Protocol"
    },
    {
      "name": "Linear Probing Task"
    },
    {
      "name": "Trajectory"
    },
    {
      "name": "Transition Model"
    },
    {
      "name": "Latent Variable"
    },
    {
      "name": "Posterior Distribution"
    },
    {
      "name": "Prior Distribution"
    },
    {
      "name": "Barlow Balancing Technique"
    },
    {
      "name": "KL Loss Function"
    },
    {
      "name": "Aggregate Metric"
    },
    {
      "name": "Encoder"
    },
    {
      "name": "Decoder"
    },
    {
      "name": "Target Network"
    },
    {
      "name": "Online Network"
    },
    {
      "name": "Bootstrapped Confidence Interval"
    },
    {
      "name": "Reward Probing Method"
    },
    {
      "name": "Predictive Belief Representation"
    },
    {
      "name": "Discrete World Model"
    },
    {
      "name": "Auxiliary Task"
    },
    {
      "name": "Batch Reinforcement Learning Method"
    },
    {
      "name": "Contrastive Learning Method"
    },
    {
      "name": "Proximal Point Algorithm"
    },
    {
      "name": "Majorization-Minimization Optimization Method"
    },
    {
      "name": "Deep Infomax Representation Learning Method"
    },
    {
      "name": "Visual Reasoning Method"
    },
    {
      "name": "Residual Block"
    },
    {
      "name": "Multi-Layer Perceptron"
    },
    {
      "name": "Gated Recurrent Unit"
    },
    {
      "name": "Projection Module"
    },
    {
      "name": "Inverse Dynamics Model"
    },
    {
      "name": "Posterior Model"
    },
    {
      "name": "Permutation Testing Method"
    },
    {
      "name": "Expert Agent"
    },
    {
      "name": "Sub-Manager"
    },
    {
      "name": "Super-Manager"
    },
    {
      "name": "Agent"
    },
    {
      "name": "Task"
    },
    {
      "name": "State Space"
    },
    {
      "name": "Human Prior"
    },
    {
      "name": "Dataset"
    },
    {
      "name": "Competition"
    },
    {
      "name": "Simulator"
    },
    {
      "name": "Food"
    },
    {
      "name": "Tool"
    },
    {
      "name": "Craftable Item"
    },
    {
      "name": "Precious Resource"
    },
    {
      "name": "Code Repository"
    },
    {
      "name": "Evaluation Environment"
    },
    {
      "name": "Compute Resource"
    },
    {
      "name": "Competition Milestone"
    },
    {
      "name": "Advisory Committee"
    },
    {
      "name": "Affinity Group"
    },
    {
      "name": "Mailing List"
    },
    {
      "name": "Media Organization"
    },
    {
      "name": "Conference"
    },
    {
      "name": "Organizer"
    },
    {
      "name": "Person"
    },
    {
      "name": "Organization"
    },
    {
      "name": "Prize"
    },
    {
      "name": "Scholarship"
    },
    {
      "name": "Travel Grant"
    },
    {
      "name": "Facility Resource"
    },
    {
      "name": "Proper Value Equivalence Model"
    },
    {
      "name": "Policy"
    },
    {
      "name": "Value Function"
    },
    {
      "name": "Bellman Operator"
    },
    {
      "name": "Value Equivalence Class"
    },
    {
      "name": "Deterministic Policy"
    },
    {
      "name": "Stochastic Policy"
    },
    {
      "name": "Order-k Value Equivalence"
    },
    {
      "name": "Tree Search Algorithm"
    },
    {
      "name": "Value Equivalence Principle"
    },
    {
      "name": "State Aggregation"
    },
    {
      "name": "Ring Markov Decision Process"
    },
    {
      "name": "False-ring Markov Decision Process"
    },
    {
      "name": "n-step Bellman Operator"
    },
    {
      "name": "MDP Homomorphism"
    },
    {
      "name": "Contraction Mapping"
    },
    {
      "name": "Fixed Point"
    },
    {
      "name": "Optimizer"
    },
    {
      "name": "Dimensionality Reduction Technique"
    },
    {
      "name": "Policy Iteration Algorithm"
    },
    {
      "name": "Distributed Architecture"
    },
    {
      "name": "Model Capacity Constraint"
    },
    {
      "name": "PVE Loss Function"
    },
    {
      "name": "Transformer Model"
    },
    {
      "name": "Reward Predictor"
    },
    {
      "name": "Composite Reward Function"
    },
    {
      "name": "Language Model"
    },
    {
      "name": "Long Short-Term Memory Network"
    },
    {
      "name": "Credit Assignment Algorithm"
    },
    {
      "name": "Bias Correction Technique"
    },
    {
      "name": "Buffer Updating Method"
    },
    {
      "name": "True Model"
    },
    {
      "name": "Value Polytope"
    },
    {
      "name": "State Aggregation Method"
    },
    {
      "name": "Approximate Model"
    },
    {
      "name": "Representational Space"
    },
    {
      "name": "Exponential Family Model"
    },
    {
      "name": "State Function"
    },
    {
      "name": "Partially Observable Markov Decision Process"
    },
    {
      "name": "Matrix"
    },
    {
      "name": "Vector"
    },
    {
      "name": "Vector Space"
    },
    {
      "name": "Lemma"
    },
    {
      "name": "Proposition"
    },
    {
      "name": "Property"
    },
    {
      "name": "Basis"
    },
    {
      "name": "Tabular Model"
    },
    {
      "name": "Rank-Constrained Matrix Factorization"
    },
    {
      "name": "Transition Matrix"
    },
    {
      "name": "Neural Network Model"
    },
    {
      "name": "Deep Q-Network"
    },
    {
      "name": "Least Squares Temporal-Difference Learning Method"
    },
    {
      "name": "Clustering Algorithm"
    },
    {
      "name": "Representation Conversion Function"
    },
    {
      "name": "Stochastic Function Approximation Method"
    },
    {
      "name": "Hyperparameter"
    },
    {
      "name": "Performance Metric"
    },
    {
      "name": "History-Based Feature Abstraction"
    },
    {
      "name": "Approximate Information State"
    },
    {
      "name": "Reproducing Kernel Hilbert Space"
    },
    {
      "name": "Finite State Machine"
    },
    {
      "name": "AIS Approximator"
    },
    {
      "name": "Actor Critic Algorithm"
    },
    {
      "name": "Multi-timescale Stochastic Gradient Ascent Algorithm"
    },
    {
      "name": "Wasserstein Distance"
    },
    {
      "name": "Energy Distance"
    },
    {
      "name": "Approximation Error"
    },
    {
      "name": "History Compression Function"
    },
    {
      "name": "Bisimulation Metric"
    },
    {
      "name": "World Model"
    },
    {
      "name": "Basis Function"
    },
    {
      "name": "Working Memory Graph"
    },
    {
      "name": "Locally Weighted Polynomial Regression Method"
    },
    {
      "name": "Kernel-based Reinforcement Learning Method"
    },
    {
      "name": "Memory-based Dynamic Programming Method"
    },
    {
      "name": "Laplacian Method"
    },
    {
      "name": "Neural Episodic Control"
    },
    {
      "name": "Average-Reward Reinforcement Learning Method"
    },
    {
      "name": "Integral Probability Metric"
    },
    {
      "name": "Minkowski Functional"
    },
    {
      "name": "Lipschitz Function"
    },
    {
      "name": "Total Variation Distance"
    },
    {
      "name": "Kernel Function"
    },
    {
      "name": "Maximum Mean Discrepancy"
    },
    {
      "name": "Behavioral Cloning Method"
    },
    {
      "name": "Regularized Behavioral Cloning Algorithm"
    },
    {
      "name": "Discriminator Model"
    },
    {
      "name": "Constant Reward Mechanism"
    },
    {
      "name": "SQIL Algorithm"
    },
    {
      "name": "Generative Adversarial Imitation Learning Algorithm"
    },
    {
      "name": "Absorbing State"
    },
    {
      "name": "Soft Value Function"
    },
    {
      "name": "Experience Replay Buffer"
    },
    {
      "name": "Policy Optimization from Demonstration Method"
    },
    {
      "name": "Occupancy Measure"
    },
    {
      "name": "Expert Demonstration"
    },
    {
      "name": "Jensen-Shannon Divergence"
    },
    {
      "name": "Adversarial Training Method"
    },
    {
      "name": "Expert Policy"
    },
    {
      "name": "Actor-Critic Framework"
    },
    {
      "name": "Sparse Reward Environment"
    },
    {
      "name": "Dynamic Reward Reshaping Algorithm"
    },
    {
      "name": "Data Collection Methodology"
    },
    {
      "name": "Data Collection Platform"
    },
    {
      "name": "Data Processing Pipeline"
    },
    {
      "name": "Software Plugin"
    },
    {
      "name": "Annotation API"
    },
    {
      "name": "Biome"
    },
    {
      "name": "Goal"
    },
    {
      "name": "Episode"
    },
    {
      "name": "Reward Function"
    },
    {
      "name": "Action Primitive"
    },
    {
      "name": "Generative Adversarial Network"
    },
    {
      "name": "State Association Learning Method"
    },
    {
      "name": "Synthetic Return"
    },
    {
      "name": "Trigger State"
    },
    {
      "name": "Temporal Value Transport Mechanism"
    },
    {
      "name": "Gating Function"
    },
    {
      "name": "SA-learning Method"
    },
    {
      "name": "Additive Regression Model"
    },
    {
      "name": "Multi-Stage Reward Decomposition Technique"
    },
    {
      "name": "Example-Based Control"
    },
    {
      "name": "Recursive Classification"
    },
    {
      "name": "Robust Example-Based Control"
    },
    {
      "name": "Success Example"
    },
    {
      "name": "State Occupancy Measure"
    },
    {
      "name": "Future Success Classifier"
    },
    {
      "name": "Bayes-optimal Classifier"
    },
    {
      "name": "Temporal Difference Method"
    },
    {
      "name": "Value Iteration Algorithm"
    },
    {
      "name": "Optimal Adversary"
    },
    {
      "name": "Classifier Model"
    },
    {
      "name": "Centralized Training with Decentralized Execution Paradigm"
    },
    {
      "name": "Decentralized Partially Observable Sequence-Markov Decision Process"
    },
    {
      "name": "Critical State"
    },
    {
      "name": "Regularization Technique"
    },
    {
      "name": "Bias"
    },
    {
      "name": "Variance"
    },
    {
      "name": "Estimator"
    },
    {
      "name": "Gaussian Distribution"
    },
    {
      "name": "Lipschitz Constant"
    },
    {
      "name": "Graph Convolution Network"
    },
    {
      "name": "Multi-head Attention Mechanism"
    },
    {
      "name": "Permutation Invariant Critic"
    },
    {
      "name": "Agent-Temporal Attention Block"
    },
    {
      "name": "Regularization Loss Function"
    },
    {
      "name": "Delayed Reward Mechanism"
    },
    {
      "name": "Object-Oriented Markov Decision Process"
    },
    {
      "name": "Meta Controller"
    },
    {
      "name": "Controller"
    },
    {
      "name": "Internal Critic"
    },
    {
      "name": "Actor Critic Method"
    },
    {
      "name": "Deep Q-Network"
    },
    {
      "name": "Extrinsic Reward"
    },
    {
      "name": "Model Parameter"
    },
    {
      "name": "Loss Function"
    },
    {
      "name": "Graph Partitioning Method"
    },
    {
      "name": "Generative Model"
    },
    {
      "name": "Relational Markov Decision Process"
    },
    {
      "name": "Factored Markov Decision Process"
    },
    {
      "name": "Object-Oriented Representation"
    },
    {
      "name": "Variational Information Maximisation Method"
    },
    {
      "name": "Universal Value Function Approximator"
    },
    {
      "name": "Cognitive Map"
    },
    {
      "name": "Semi-Markov Decision Process"
    }
  ],
  "relationships": [
    {
      "name": "transforms"
    },
    {
      "name": "enables"
    },
    {
      "name": "yields"
    },
    {
      "name": "addresses"
    },
    {
      "name": "decomposes"
    },
    {
      "name": "outperforms"
    },
    {
      "name": "maps"
    },
    {
      "name": "decouples"
    },
    {
      "name": "inspires"
    },
    {
      "name": "modulates"
    },
    {
      "name": "integrates"
    },
    {
      "name": "transfers"
    },
    {
      "name": "complements"
    },
    {
      "name": "correlates"
    },
    {
      "name": "bridges"
    },
    {
      "name": "minimizes"
    },
    {
      "name": "pushes"
    },
    {
      "name": "regularizes"
    },
    {
      "name": "improves"
    },
    {
      "name": "guides"
    },
    {
      "name": "projects"
    },
    {
      "name": "shrinks"
    },
    {
      "name": "controls"
    },
    {
      "name": "instructs"
    },
    {
      "name": "delegates"
    },
    {
      "name": "bypasses"
    },
    {
      "name": "leverages"
    },
    {
      "name": "catalyzes"
    },
    {
      "name": "is prerequisite for"
    },
    {
      "name": "crafted from"
    },
    {
      "name": "obtains"
    },
    {
      "name": "forks"
    },
    {
      "name": "partners with"
    },
    {
      "name": "promotes"
    },
    {
      "name": "affiliated with"
    },
    {
      "name": "advised by"
    },
    {
      "name": "awards"
    },
    {
      "name": "provides"
    },
    {
      "name": "generalizes"
    },
    {
      "name": "converges_to"
    },
    {
      "name": "approximates"
    },
    {
      "name": "grows_with"
    },
    {
      "name": "balances"
    },
    {
      "name": "mimics"
    },
    {
      "name": "constrains"
    },
    {
      "name": "parameterizes"
    },
    {
      "name": "overlays"
    },
    {
      "name": "adopts"
    },
    {
      "name": "conditions on"
    },
    {
      "name": "induces"
    },
    {
      "name": "specializes to"
    },
    {
      "name": "explains"
    },
    {
      "name": "validates"
    },
    {
      "name": "implies"
    },
    {
      "name": "subset_of"
    },
    {
      "name": "aggregates"
    },
    {
      "name": "reports"
    },
    {
      "name": "characterizes"
    },
    {
      "name": "updates"
    },
    {
      "name": "extends"
    },
    {
      "name": "bounds"
    },
    {
      "name": "is equivalent to"
    },
    {
      "name": "incorporates"
    },
    {
      "name": "relies on"
    },
    {
      "name": "derived from"
    },
    {
      "name": "enforces"
    },
    {
      "name": "overcomes"
    },
    {
      "name": "optimizes"
    },
    {
      "name": "annotates"
    },
    {
      "name": "captures"
    },
    {
      "name": "overlaps with"
    },
    {
      "name": "precedes"
    },
    {
      "name": "builds upon"
    },
    {
      "name": "combines"
    },
    {
      "name": "augments"
    },
    {
      "name": "predicts"
    },
    {
      "name": "assigns_credit_to"
    },
    {
      "name": "incentivizes"
    },
    {
      "name": "coincides_with"
    },
    {
      "name": "measures"
    },
    {
      "name": "estimates"
    },
    {
      "name": "replaces"
    },
    {
      "name": "satisfies"
    },
    {
      "name": "corresponds to"
    },
    {
      "name": "assigns_weight_to"
    },
    {
      "name": "is fixed point of"
    },
    {
      "name": "modifies"
    },
    {
      "name": "redistributes"
    },
    {
      "name": "analyzes"
    },
    {
      "name": "influences"
    },
    {
      "name": "concatenates"
    },
    {
      "name": "infers"
    },
    {
      "name": "return-equivalent"
    },
    {
      "name": "shares"
    },
    {
      "name": "compares"
    },
    {
      "name": "operates_at"
    },
    {
      "name": "selects"
    },
    {
      "name": "reaches"
    }
  ]
}